{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Главы 11 - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\n', 'story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \\n']\n",
      "['positive\\n', 'negative\\n']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# скачиваем отзывы\n",
    "raw_reviews = []\n",
    "with open('reviews.txt') as f:\n",
    "    for raw in f.readlines():\n",
    "        raw_reviews.append(raw)\n",
    "\n",
    "# скачиваем метки\n",
    "raw_labels = []\n",
    "with open('labels.txt') as f:\n",
    "    for label in f.readlines():\n",
    "        raw_labels.append(label)\n",
    "\n",
    "print(raw_reviews[:2])\n",
    "print(raw_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens):25000\n",
      "type(tokens) <class 'list'>\n",
      "type(tokens[0]) <class 'set'>\n"
     ]
    }
   ],
   "source": [
    "# собираем токены в список множеств\n",
    "tokens = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
    "print(f'len(tokens):{len(tokens)}\\ntype(tokens) {type(tokens)}\\ntype(tokens[0]) {type(tokens[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74074"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем словарь, очищаем от пустых строк\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if len(word)>0:\n",
    "            vocab.add(word)\n",
    "# делаем из множества список\n",
    "vocab = list(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74074"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пронумеруем каждое слово и поместим в словарь\n",
    "word2index = {}\n",
    "for n,w in enumerate(vocab):\n",
    "    word2index[w] = n\n",
    "\n",
    "len(word2index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# формируем массив для модели (предикторы)\n",
    "input_dataset = []\n",
    "for sent in tokens:\n",
    "    sent_indices = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            continue\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "# получили список из списков индексов для каждого отзыва\n",
    "len(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# формируем массив для модели (целевая переменная)\n",
    "target_dataset = []\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)\n",
    "\n",
    "# получили список меток (1 - положительный, 0 - отрицательный)\n",
    "len(target_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторное представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разберемся с векторным представлением\n",
    "import numpy as np\n",
    "aa = np.arange(1,16).reshape(3,5)\n",
    "# матрица весов\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = np.ones(3, dtype=int).reshape(-1,3)\n",
    "bb[:,1] = 0\n",
    "# один вектор наблюдений из нулей и единиц\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 14, 16, 18, 20]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# скалярное произведение это сумма по столбцам без учета строк \n",
    "# номера которых равны индексам нулей в векторе наблюдений\n",
    "bb.dot(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 14, 16, 18, 20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(aa[[0,2],:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# То есть не надо делать дорогие матричные перемножения, достаточно сделать сложение! Особенно когда вектор длиной 74074.\n",
    "# отразим это векторное представление в коде (для входного слоя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 Progress: 0.0 % Training Acc: 0.0%\n",
      "Iter: 0 Progress: 12.0 % Training Acc: 40.453%\n",
      "Iter: 0 Progress: 24.0 % Training Acc: 57.057%\n",
      "Iter: 0 Progress: 36.0 % Training Acc: 64.637%\n",
      "Iter: 0 Progress: 48.0 % Training Acc: 68.728%\n",
      "Iter: 0 Progress: 60.0 % Training Acc: 71.089%\n",
      "Iter: 0 Progress: 72.0 % Training Acc: 72.468%\n",
      "Iter: 0 Progress: 84.0 % Training Acc: 73.973%\n",
      "Iter: 0 Progress: 96.0 % Training Acc: 75.129%\n",
      "\n",
      "Iter: 1 Progress: 0.0 % Training Acc: 75.13%\n",
      "Iter: 1 Progress: 12.0 % Training Acc: 76.345%\n",
      "Iter: 1 Progress: 24.0 % Training Acc: 77.264%\n",
      "Iter: 1 Progress: 36.0 % Training Acc: 78.119%\n",
      "Iter: 1 Progress: 48.0 % Training Acc: 78.892%\n",
      "Iter: 1 Progress: 60.0 % Training Acc: 79.506%\n",
      "Iter: 1 Progress: 72.0 % Training Acc: 79.993%\n",
      "Iter: 1 Progress: 84.0 % Training Acc: 80.532%\n",
      "Iter: 1 Progress: 96.0 % Training Acc: 80.95%\n",
      "\n",
      "Iter: 2 Progress: 0.0 % Training Acc: 80.95%\n",
      "Iter: 2 Progress: 12.0 % Training Acc: 81.438%\n",
      "Iter: 2 Progress: 24.0 % Training Acc: 81.784%\n",
      "Iter: 2 Progress: 36.0 % Training Acc: 82.156%\n",
      "Iter: 2 Progress: 48.0 % Training Acc: 82.522%\n",
      "Iter: 2 Progress: 60.0 % Training Acc: 82.875%\n",
      "Iter: 2 Progress: 72.0 % Training Acc: 83.152%\n",
      "Iter: 2 Progress: 84.0 % Training Acc: 83.455%\n",
      "Iter: 2 Progress: 96.0 % Training Acc: 83.725%\n",
      "\n",
      "Test Acc: 81.5% \n",
      " ------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
    "alpha, iterations, hidden_size = 0.01, 3, 100\n",
    "\n",
    "weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1 # (74074,100)\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1 # (100,1)\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    for i in range(len(input_dataset) - 1000): # для обучения берем первые 24 000 примеров\n",
    "        x, y = (input_dataset[i], target_dataset[i])\n",
    "        \n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0).reshape(1,-1)) # (1,100)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2)) # (1,100)dot(100,1)=(1,1)\n",
    "        \n",
    "        layer_2_delta = layer_2 - y # (1,1)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) # (1,1)dot(1,100)=(1,100)\n",
    "        \n",
    "        weights_1_2 -= layer_1.T * layer_2_delta * alpha\n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        \n",
    "        if np.abs(layer_2_delta) < 0.4: # если абсолютная ошибка меньше 0,5, то засчитываем как правильный ответ\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if i % 3000 == 0 or i == (len(input_dataset) - 1000 - 1):\n",
    "            progress = 100*i/len(input_dataset)\n",
    "            print(f\"Iter: {iteration} Progress: {round(progress, 2)} % Training Acc: {round(100*correct/total, 3)}%\")\n",
    "    print()\n",
    "\n",
    "# проверка на тесте\n",
    "correct, total = 0, 0\n",
    "for i in range(len(input_dataset)-1000, len(input_dataset)):\n",
    "    \n",
    "    x, y = input_dataset[i], target_dataset[i]\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0)) # (1,100)\n",
    "    layer_2 = sigmoid(layer_1.dot(weights_1_2)) # (1,100)dot(100,1)=(1,1)\n",
    "    if np.abs(layer_2 - y) < 0.4: # если абсолютная ошибка меньше 0,5, то засчитываем как правильный ответ\n",
    "            correct += 1\n",
    "    total += 1\n",
    "print(f'Test Acc: {round(100*correct/total, 5)}%', '\\n', '-'*30)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение векторных представлений (весов) слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно выбрать слово, посмотреть на его вектор весов во входном слое и поискать близкие векторы.\n",
    "Сравнимаем строки в матрице весов!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# import math\n",
    "\n",
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target] # индекс слова в мешке слов\n",
    "    scores = Counter() # словарь для учета частот слов\n",
    "    for word, index in word2index.items(): # бежим по ключам, значениям мешка слов\n",
    "        raw_difference = weights_0_1[index] - weights_0_1[target_index]\n",
    "        scores[word] = -1 * np.sqrt(np.sum(raw_difference**2)) # добавляем с минусом евклидово расстояние\n",
    "        \n",
    "    return scores.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beautiful', -0.0), ('tight', -0.7209523658054507), ('realistic', -0.7236111385216073), ('flawless', -0.726916481712693), ('notch', -0.7335271507538228), ('negative', -0.7411976375234283), ('awesome', -0.7428778479027082), ('treat', -0.7455946929079215), ('worlds', -0.7478677576454998), ('impressed', -0.7504621838874066)] \n",
      "\n",
      "[('terrible', -0.0), ('fails', -0.756374397231069), ('avoid', -0.7991837573351354), ('wooden', -0.8007299958561985), ('boring', -0.8087608383893226), ('badly', -0.815500442396233), ('redeeming', -0.8338771177322658), ('horrible', -0.8388270767871248), ('annoying', -0.8405398955631521), ('forgettable', -0.8432361209217222)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in ['beautiful', 'terrible']:\n",
    "    print(similar(target=w), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', -0.0),\n",
       " ('tight', -0.7209523658054507),\n",
       " ('realistic', -0.7236111385216073),\n",
       " ('flawless', -0.726916481712693),\n",
       " ('notch', -0.7335271507538228),\n",
       " ('negative', -0.7411976375234283),\n",
       " ('awesome', -0.7428778479027082),\n",
       " ('treat', -0.7455946929079215),\n",
       " ('worlds', -0.7478677576454998),\n",
       " ('impressed', -0.7504621838874066)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(target='beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', -0.0),\n",
       " ('fails', -0.756374397231069),\n",
       " ('avoid', -0.7991837573351354),\n",
       " ('wooden', -0.8007299958561985),\n",
       " ('boring', -0.8087608383893226),\n",
       " ('badly', -0.815500442396233),\n",
       " ('redeeming', -0.8338771177322658),\n",
       " ('horrible', -0.8388270767871248),\n",
       " ('annoying', -0.8405398955631521),\n",
       " ('forgettable', -0.8432361209217222)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(target='terrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atmosphere', -0.0),\n",
       " ('powerful', -0.691491458151142),\n",
       " ('sweet', -0.7029953285676174),\n",
       " ('outstanding', -0.7084380957648823),\n",
       " ('beautifully', -0.7118115346774474),\n",
       " ('liked', -0.7122365707933771),\n",
       " ('tight', -0.729463536913003),\n",
       " ('episodes', -0.7376865613748727),\n",
       " ('criticism', -0.741091737358788),\n",
       " ('spectacular', -0.7415743885841557)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(target='atmosphere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сеть группирует слова через призму \"положительный или отрицательный отзыв\". \n",
    "# Смысл нейрона определяется предсказываемыми метками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0% [('terrible', -0.0), ('weened', -0.3684632319655597), ('dullllllllllll', -0.38338063344860374), ('memories', -0.3848793731367956), ('preaches', -0.38579367148753596), ('deadhead', -0.3913601228910714), ('janet', -0.3915047699956218), ('flamethrowers', -0.3947111977040416), ('jermy', -0.39762091865648075), ('exceptionally', -0.397740171574861)]\n",
      "Progress: 5.0% [('terrible', -0.0), ('fantastic', -1.8009723152804034), ('perfectly', -1.8084681166926186), ('ridiculous', -1.810893990647885), ('horrible', -1.8534342521428562), ('essentially', -1.8558070053929663), ('solid', -1.9858713078140156), ('okay', -1.9984142687261657), ('cute', -2.004782985413128), ('carrying', -2.011581502113405)]\n",
      "Progress: 10.0% [('terrible', -0.0), ('brilliant', -2.0549438356875007), ('fantastic', -2.055777101022888), ('unique', -2.067346433621027), ('magnificent', -2.166955632968605), ('manner', -2.1923149174754), ('l', -2.1990597551875735), ('student', -2.2130714789228394), ('teenager', -2.217612808655398), ('competent', -2.2224567944894837)]\n",
      "Progress: 15.0% [('terrible', -0.0), ('lame', -2.53941943296039), ('brilliant', -2.7272789120591616), ('fantastic', -2.8462729310800765), ('horrible', -2.8871576608829503), ('ridiculous', -3.0234729283285993), ('hilarious', -3.025955489976737), ('dumb', -3.0826313503951197), ('magnificent', -3.132327320777336), ('awful', -3.1672071764301126)]\n",
      "Progress: 20.0% [('terrible', -0.0), ('horrible', -2.966288754654569), ('brilliant', -3.273846128016254), ('fantastic', -3.308579667553651), ('lame', -3.552856178383435), ('hilarious', -3.7054828274959024), ('superb', -3.7326301557260613), ('remarkable', -3.774954572805489), ('compelling', -3.8125787904066706), ('magnificent', -3.823451044939597)]\n",
      "Progress: 25.0% [('terrible', -0.0), ('horrible', -2.8955652523470428), ('brilliant', -3.1680543620542836), ('lame', -3.521001996009623), ('fantastic', -3.557280090361208), ('great', -3.6716165827034795), ('superb', -3.830385593473635), ('remarkable', -3.89204475378391), ('hilarious', -3.940361978931815), ('boring', -3.9536439163479673)]\n",
      "Progress: 30.0% [('terrible', -0.0), ('horrible', -3.345234484003494), ('mediocre', -3.6686510775864467), ('fantastic', -3.7032794599885284), ('laughable', -3.8637093658260797), ('pathetic', -3.896263799862258), ('marvelous', -3.905137644248811), ('weak', -3.9083393712616106), ('dreadful', -3.9315575377921714), ('lame', -3.9384266027771697)]\n",
      "Progress: 35.0% [('terrible', -0.0), ('horrible', -3.095413593654045), ('brilliant', -3.692839682309019), ('lame', -3.882260031249786), ('pathetic', -3.914082155497125), ('weak', -4.016069547085188), ('laughable', -4.033460548571598), ('mediocre', -4.086486102552377), ('wonderful', -4.201851807107519), ('ridiculous', -4.237548701659223)]\n",
      "Progress: 40.0% [('terrible', -0.0), ('horrible', -2.850901498437856), ('brilliant', -3.5454130715556094), ('superb', -3.790706499129875), ('pathetic', -3.7912355270601936), ('bad', -3.8464854433876714), ('lame', -3.8526917529565976), ('mediocre', -3.902866682709402), ('laughable', -3.951835824809987), ('wonderful', -3.9578226282138216)]\n",
      "Progress: 45.0% [('terrible', -0.0), ('horrible', -2.898734433119978), ('brilliant', -3.223803161138777), ('superb', -3.4343140457280104), ('bad', -3.7600989031044416), ('fantastic', -3.7767209936758284), ('pathetic', -3.782750948475709), ('wonderful', -3.8183313614982404), ('terrific', -3.926500769349846), ('laughable', -3.9701981800397683)]\n",
      "Progress: 50.0% [('terrible', -0.0), ('horrible', -3.2038530049552865), ('superb', -3.4014771199470286), ('brilliant', -3.403393169137575), ('pathetic', -3.709164958097723), ('fantastic', -3.737170710718821), ('lame', -3.867588132304824), ('great', -3.9868173692325866), ('hilarious', -4.160007148222998), ('stupid', -4.16611049903272)]\n",
      "Progress: 55.0% [('terrible', -0.0), ('horrible', -3.171950154721256), ('superb', -3.3340630247332217), ('brilliant', -3.6708360863753913), ('ridiculous', -3.692976354196554), ('laughable', -3.7815910765273673), ('fantastic', -3.8372783020541146), ('pathetic', -3.9118743728816687), ('awful', -4.009712633668116), ('haunting', -4.048725560061436)]\n",
      "Progress: 60.0% [('terrible', -0.0), ('horrible', -2.991979070435913), ('fantastic', -3.16184549130396), ('brilliant', -3.476163714613624), ('lame', -3.7506821733049285), ('pathetic', -3.7641943715363086), ('superb', -3.7692217378388126), ('magnificent', -3.81841294057524), ('ridiculous', -3.888170423493976), ('marvelous', -3.9244349111106502)]\n",
      "Progress: 65.0% [('terrible', -0.0), ('horrible', -2.767107799309289), ('ridiculous', -3.680839519606267), ('fantastic', -3.734683754231092), ('pathetic', -3.745438093207623), ('brilliant', -3.751665832686506), ('lame', -3.75361465960698), ('laughable', -3.8068666336237467), ('dreadful', -3.971683791597918), ('fabulous', -4.004036498718303)]\n",
      "Progress: 70.0% [('terrible', -0.0), ('horrible', -2.4432273011463033), ('brilliant', -3.4504122902075043), ('magnificent', -3.71313094730808), ('superb', -3.851864319834549), ('fantastic', -3.868239755205578), ('lame', -3.9768447723670453), ('pathetic', -3.999841039578111), ('dreadful', -4.022933451325269), ('horrendous', -4.062047222583562)]\n",
      "Progress: 75.0% [('terrible', -0.0), ('horrible', -2.593038113005263), ('brilliant', -3.510444722225508), ('fantastic', -3.934197836933132), ('pathetic', -4.017660543542843), ('wonderful', -4.070551129990966), ('marvelous', -4.187868265190884), ('magnificent', -4.270928263407441), ('dreadful', -4.309896092053277), ('horrendous', -4.3223011462649366)]\n",
      "Progress: 80.0% [('terrible', -0.0), ('horrible', -3.154858807671039), ('dire', -3.765299046515452), ('brilliant', -3.8233811862824676), ('dreadful', -3.859649301826047), ('laughable', -3.9358617876901265), ('fantastic', -3.9550613877545775), ('marvelous', -3.968646381372185), ('fabulous', -3.9709659225068163), ('superb', -4.039399428741782)]\n",
      "Progress: 85.0% [('terrible', -0.0), ('horrible', -3.226185707327507), ('brilliant', -3.479670611068968), ('dreadful', -3.7579872369780527), ('horrendous', -3.812923901867089), ('horrid', -3.8628545661420866), ('great', -3.8711426738159225), ('lame', -3.8784147779468916), ('dire', -3.928614488437423), ('laughable', -4.009533574766157)]\n",
      "Progress: 90.0% [('terrible', -0.0), ('horrible', -2.9637539308791716), ('brilliant', -3.282587287072819), ('dreadful', -3.7495273546255516), ('horrendous', -3.7875442524952665), ('bad', -3.8137500019160453), ('marvelous', -3.848847137247472), ('dire', -3.988711843004451), ('magnificent', -4.0111343168694304), ('phenomenal', -4.018308939826204)]\n",
      "Progress: 95.0% [('terrible', -0.0), ('horrible', -3.066401882308291), ('brilliant', -3.4428976908740805), ('superb', -3.6499526243337086), ('pathetic', -3.7469189176604876), ('bad', -3.870780859984387), ('phenomenal', -3.9250451907395605), ('horrendous', -3.9401810839055065), ('stupid', -3.9564690368655313), ('dreadful', -4.073275342775358)]\n",
      "[('terrible', -0.0), ('horrible', -2.9609516319775397), ('brilliant', -3.4227483323400736), ('pathetic', -3.8087419642609412), ('superb', -3.8681942864246763), ('bad', -3.9031091570118384), ('masterful', -3.9591557474366144), ('phenomenal', -3.9653591182781738), ('marvelous', -4.004978686580678), ('mediocre', -4.192722139433791)]\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "tokens = list(map(lambda x: (x.split(\" \")), raw_reviews))\n",
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0], wordcnt.most_common())))\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "concatenated = []\n",
    "input_dataset = []\n",
    "\n",
    "for sent in tokens:\n",
    "    sent_indices = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            continue\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "\n",
    "random.shuffle(input_dataset) # Берет последовательность и возвращает ее в перемешанном состоянии\n",
    "alpha, iterations, hidden_size, window, negative = 0.05, 2, 50, 2, 5\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab), hidden_size) * 0\n",
    "\n",
    "layer_2_target = np.zeros(negative + 1)\n",
    "layer_2_target[0] = 1\n",
    "\n",
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target] # индекс слова в мешке слов\n",
    "    scores = Counter() # словарь для учета частот слов\n",
    "    for word, index in word2index.items(): # бежим по ключам, значениям мешка слов\n",
    "        raw_difference = weights_0_1[index] - weights_0_1[target_index]\n",
    "        scores[word] = -1 * np.sqrt(np.sum(raw_difference**2)) # добавляем с минусом евклидово расстояние\n",
    "        \n",
    "    return scores.most_common(10)\n",
    "\n",
    "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
    "\n",
    "for rev_i, review in enumerate(input_dataset * iterations):\n",
    "    for target_i in range(len(review)):\n",
    "        \n",
    "        target_samples = [review[target_i]] + list(concatenated\\\n",
    "                                                   [(np.random.rand(negative) * len(concatenated)).astype('int').tolist()])\n",
    "        left_context = review[max(0, target_i - window):target_i]\n",
    "        right_context = review[target_i + 1 : min(len(review), target_i + window)]\n",
    "        \n",
    "        layer_1 = np.mean(weights_0_1[left_context + right_context], axis=0)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        \n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "        \n",
    "        weights_0_1[left_context + right_context] -= layer_1_delta * alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta, layer_1) * alpha\n",
    "        \n",
    "    if rev_i % 2500 == 0 :\n",
    "        progress = 100*rev_i/(len(input_dataset)*iterations)\n",
    "        print(f\"Progress: {round(progress, 2)}% {similar('terrible')}\")\n",
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weights_part_4', [weights_0_1, weights_1_2])\n",
    "# np.save('weights_part_4', weights_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 74075, 50)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('weights_part_4.npy').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словесная аналогия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive=['terrible', 'good'], negative=['bad']):\n",
    "    \n",
    "    norms = np.sum(weights_0_1 * weights_0_1, axis = 1)\n",
    "    norms.resize(norms.shape[0], 1)\n",
    "    \n",
    "    normed_weights = weights_0_1 * norms\n",
    "    \n",
    "    query_vect = np.zeros(len(weights_0_1[0]))\n",
    "    \n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[word2index[word]]\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[word2index[word]]\n",
    "        \n",
    "    scores = Counter()\n",
    "    \n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - query_vect\n",
    "        squared_difference = raw_difference ** 2\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "        \n",
    "    return scores.most_common(10)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superb', -221.46820338343645),\n",
       " ('terrific', -221.8148593701283),\n",
       " ('decent', -221.90054276067337),\n",
       " ('fine', -221.95167472296896),\n",
       " ('perfect', -222.19214848996603),\n",
       " ('nice', -222.28298362559514),\n",
       " ('great', -222.3216032822223),\n",
       " ('worth', -222.3327471074601),\n",
       " ('brilliant', -222.35552439378316)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'terrible' - 'bad' + 'good'\n",
    "analogy(['terrible', 'good'], ['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christopher', -198.31092886508438),\n",
       " ('mr', -198.9767936617917),\n",
       " ('john', -199.2148675518587),\n",
       " ('de', -199.23572493921867),\n",
       " ('it', -199.25465723045528),\n",
       " ('william', -199.2559490448937),\n",
       " ('him', -199.29416405936328),\n",
       " ('tom', -199.35824539168513),\n",
       " ('david', -199.3807888925693)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'elizabeth' - 'she' + 'he'\n",
    "analogy(['elizabeth', 'he'], ['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усредняем вектора слов и предложений (глава 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpikulev\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\lpikulev\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan',\n",
       " 'brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love sce',\n",
       " 'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life',\n",
       " 'airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belongin',\n",
       " 'story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terr']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "norms = np.sum(weights_0_1 ** 2, axis=1) # суммируем по горизонтали по каждой строке, т.е. \n",
    "                                        # для каждого слова его веса возводим в квадрат (делаем положительными)\n",
    "                                        # и суммируем\n",
    "norms = norms.reshape(-1,1) # добавляем измерение ширины 1\n",
    "normed_weights = weights_0_1 * norms # нормируем веса, домножив на положительное число\n",
    "\n",
    "def make_sent_vect(words):\n",
    "    # из словаря word2index, в котором каждому слову (ключ) присвоен номер (значение),\n",
    "    # отбираем индексы тех слова, которые есть у нас (переданы в функцию)\n",
    "    # и помещаем в список indices\n",
    "    indices = list(map(lambda x: word2index[x], filter(lambda x: x in word2index, words)))\n",
    "    # возвращаем усредненный индекс для нашего набора слов (усредняем по столбцам)\n",
    "    return np.mean(normed_weights[indices], axis=0)\n",
    "\n",
    "reviews2vectors = [] # пустой список для усредненных векторов\n",
    "for review in tokens: # берем список из отдельных слов, относящийся к одному отзыву\n",
    "    reviews2vectors.append(make_sent_vect(review)) # наполняем список\n",
    "reviews2vectors = np.array(reviews2vectors) # переводим в np массив (25000 отзывов, 50 значений усредненного вектора весов)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review) # (1,50)\n",
    "    scores = Counter()\n",
    "    for i, val in enumerate(reviews2vectors.dot(v.T)): # (25000,50) dot (50,1) = (25000,1) т.е. 25 тыс. расстояний\n",
    "        scores[i] = val # записываем в словарь для каждого отзыва вектор-столбец из 25 тыс. расстояний\n",
    "        \n",
    "        most_similar = []\n",
    "        \n",
    "        for idx, score in scores.most_common(5): # most_common возвращает кортежи\n",
    "            most_similar.append(raw_reviews[idx][:100]) # из \"сырого\" списка отзывов по индексу достаем отзыв \n",
    "                                                        # и сохраняем первые 40 символов \n",
    "    return most_similar\n",
    "\n",
    "most_similar_reviews(['boring', 'awful'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас наш алгоритм никак не учитывает порядок слов. Для того, чтобы результат зависел от порядка слов, поэкспериментируем с перемножением векторов слов на единичные матрицы (частный случай матриц перехода)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([0.1,0.2,0.3])\n",
    "c = np.array([-1,-0.5,0])\n",
    "d = np.array([0,0,0])\n",
    "\n",
    "identity = np.eye(3) # создает единичные матрицы\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "[0.1 0.2 0.3]\n",
      "[-1.  -0.5  0. ]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for v in [a,b,c,d]:\n",
    "    print(v.dot(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "this = np.array([2,4,6])\n",
    "movie = np.array([10,10,10])\n",
    "rocks = np.array([1,1,1])\n",
    "\n",
    "print(this + movie + rocks) # просто складываем векторы слов\n",
    "print((this.dot(identity) + movie).dot(identity) + rocks) # между сложением векторов слов умножаем промежуточные суммы на ед. матрицу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямое распространение на Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_) # atleast_2d сразу решейпит в (1, -1) !\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True) # keepdims=True также решейпит в (1, -1) !\n",
    "\n",
    "# векторные представления слов\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0., 0., 0.]])\n",
    "word_vects['bears'] = np.array([[0., 0., 0.]])\n",
    "word_vects['braves'] = np.array([[0., 0., 0.]])\n",
    "word_vects['red'] = np.array([[0., 0., 0.]])\n",
    "word_vects['sox'] = np.array([[0., 0., 0.]])\n",
    "word_vects['lose'] = np.array([[0., 0., 0.]])\n",
    "word_vects['defeat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['beat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['tie'] = np.array([[0., 0., 0.]])\n",
    "\n",
    "# веса для прогнозирования следующее слово по матрице предложения из 3 слов \n",
    "sent2output = np.random.rand(3, len(word_vects))\n",
    "# матрица перехода (вначале единичная)\n",
    "identity = np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямое распространение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# создаем предложение в векторном представлении\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "# делаем прогнозы\n",
    "pred = softmax(layer_2.dot(sent2output))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратное распространение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# закодируем правильный ответ (предсказание слова 'yankees')\n",
    "y = np.array([1,0,0,0,0,0,0,0,0])\n",
    "alpha = 0.01\n",
    "\n",
    "## спускаемся вниз\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "\n",
    "defeat_delta = layer_2_delta * 1 # получаем дельту для обучения матрицы (лучше отображать слово)\n",
    "layer_1_delta = layer_2_delta.dot(identity.T) # получаем дельту для обучения матрицы перехода (рекуррентной матрицы)\n",
    "\n",
    "sox_delta = layer_1_delta * 1\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "\n",
    "## поднимаемся вверх\n",
    "\n",
    "# обучаем сами вектора слов\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['sox'] -= sox_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "\n",
    "# обучаем матрицу перехода от 1 слоя к 2 и от 2 к 3\n",
    "identity -= np.outer(layer_0, layer_1_delta) # np.outer перемножает между собой элементы в одинаковых позициях\n",
    "identity -= np.outer(layer_1, layer_2_delta)\n",
    "\n",
    "# обучаем матрицу весов\n",
    "sent2output -= np.outer(layer_2, pred_delta) * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучим на датасете Babi\n",
    "import re\n",
    "regex = re.compile('[0-9?!.,\\t\\n]+')\n",
    "\n",
    "tokens = []\n",
    "with open('qa1_single-supporting-fact_train.txt', 'r') as f:\n",
    "    for raw in f.readlines()[:1000]:\n",
    "        tokens.append(regex.sub('', raw).split(\" \")[1:])\n",
    "#         tokens.append(regex.sub('', raw.lower()).split(\" \")[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mary', 'moved', 'to', 'the', 'bathroom'],\n",
       " ['John', 'went', 'to', 'the', 'hallway'],\n",
       " ['Where', 'is', 'Mary', 'bathroom']]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = []\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) # нормируем вектор (это не обязательно)\n",
    "    return e_x / e_x.sum(axis=0) # выдаем вектор, сумма элементов которого равна 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "emded_size = 10 # величина векторного представления (сколькими значениями кодируем отзыв)\n",
    "\n",
    "embed = (np.random.rand(len(vocab), emded_size) - 0.5) * 0.1 # матрица входных весов (19,10)\n",
    "recurrent = np.eye(emded_size) # рекуррентная матрица (вначале единичная, потом обучится)\n",
    "start = np.zeros(emded_size) # представление для пустого предложения\n",
    "decoder = (np.random.rand(emded_size, len(vocab)) - 0.5) * 0.1 # матрица выходных весов\n",
    "one_hot = np.eye(len(vocab)) # матрица поиска выходных весов (там 70 тыс строк и в каждой строке по единице\n",
    "                            # в позиции, соответствующей индексу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary', 'moved', 'to', 'the', 'bathroom']"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'went': 0,\n",
       " 'Mary': 1,\n",
       " 'John': 2,\n",
       " 'back': 3,\n",
       " 'to': 4,\n",
       " 'bathroom': 5,\n",
       " 'journeyed': 6,\n",
       " 'garden': 7,\n",
       " 'Daniel': 8,\n",
       " 'moved': 9,\n",
       " 'Sandra': 10,\n",
       " 'office': 11,\n",
       " 'Where': 12,\n",
       " 'kitchen': 13,\n",
       " 'bedroom': 14,\n",
       " 'the': 15,\n",
       " 'hallway': 16,\n",
       " 'is': 17,\n",
       " 'travelled': 18}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went',\n",
       " 'Mary',\n",
       " 'John',\n",
       " 'back',\n",
       " 'to',\n",
       " 'bathroom',\n",
       " 'journeyed',\n",
       " 'garden',\n",
       " 'Daniel',\n",
       " 'moved',\n",
       " 'Sandra',\n",
       " 'office',\n",
       " 'Where',\n",
       " 'kitchen',\n",
       " 'bedroom',\n",
       " 'the',\n",
       " 'hallway',\n",
       " 'is',\n",
       " 'travelled']"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    layers = [] # список слоев (коих м.б. разное количество в зависимости от числа слов в предложении)\n",
    "    layer = {} # пока пустой словарь\n",
    "    layer['pred'] = np.zeros((1,len(vocab))) # только для демонстрации записываем в словарь пустое прдсказание, далее\n",
    "                                            # будем обновлять\n",
    "    layer['hidden'] = start # в словарь записываем скрытый слой, на который передается вектор из нулей длиной embed_size\n",
    "                            # дальше будем обновлять значения по этому ключу\n",
    "    layers.append(layer) # словарь добавляем в список\n",
    "    \n",
    "    loss = 0 # начальное значение ошибки (ф-я потерь)\n",
    "        \n",
    "    for target_i in range(len(sent)): # для каждого слова из предложения\n",
    "        layer = {} # создаем новый пустой словарь\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder)) # записываем в него попытку предсказания:\n",
    "                                                                    # первым выбирается start - вектор из нулей \n",
    "                                                                    # по количеству слов в предложении (1,10),\n",
    "                                                                    # т.е. берем последний записанный словарь из списка layers\n",
    "                                                                    # и в нем по ключу 'hidden'\n",
    "                                                                    # находим вектор предложения \n",
    "                                                                    # и скалярно умножаем на матрицу выходных весов (10,19)\n",
    "                                                                    # получаем вектор (1,19)\n",
    "#         print(layer['pred'].shape)\n",
    "#         print(layer['pred'][target_i])\n",
    "        loss += -np.log(layer['pred'][sent[target_i]]) # учитываем лосс: десятичный логарифм от 0 (если ошибка) и от 1 (если верно)\n",
    "#         print(loss)\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[target_i] # обновляем значения нейронов скрытого слоя^\n",
    "                    # в первую итерацию вместо вектора из 10 нулей получаем вектор из 9 нулей и одной единицы\n",
    "                                                            \n",
    "        layers.append(layer) # список layers будет накапливать элементы, но использовать мы будем их с последнего\n",
    "    \n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратное распространение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iteration% len(tokens)][1:]) # берем очередной отзыв\n",
    "    layers, loss = predict(sent) # получаем список значений на скрытом слое и прогноз\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))): # [4, 3, 2, 1, 0]\n",
    "        layer = layers[layer_idx] # берем слой (с последнего)\n",
    "        target = sent[layer_idx - 1]\n",
    "        \n",
    "        if layer_idx > 0: # если не первый слой\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target] # сравниваем предсказание с правдой\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.T)\n",
    "            \n",
    "            if layer_idx == len(layers) - 1 :\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                layers[layer_idx+1]['hidden_delta'].dot(recurrent.T)\n",
    "        else: # если первый слой\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta']\\\n",
    "            .dot(recurrent.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.787363136429493"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПОЯСНЕНИЕ ЗАПУТАННОГО КОДА ИЗ КНИГИ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary', 'moved', 'to', 'the', 'bathroom']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ИТЕРАЦИЯ №0, вектор предложения: [9, 4, 15, 5] (без первого элемента)\n",
      "Количество слоев (len(layers)): 5\n",
      "Ошибка (loss) 11.784163594563987 \n",
      "\n",
      "Работаем со слоем 4\n",
      "Извлекаем из списка слоев слой №4\n",
      "Целевое слово (\"bathroom\") под индексом: 5\n",
      "\tЭто не самый первый слой!\n",
      "\tСравнили предсказание (layer['pred']) с правдой (one_hot[target]), получили дельту (layer['output_delta']): \n",
      "[ 0.05270767  0.05228045  0.05326576  0.05259298  0.0523218  -0.9473208\n",
      "  0.0523481   0.05251092  0.05294389  0.05287013  0.05273489  0.05229335\n",
      "  0.05270291  0.05250183  0.05254811  0.05249804  0.05264425  0.05240833\n",
      "  0.05314741]\n",
      "\tТеперь рапространим градиент (полученную дельту) на скрытый слой.\n",
      "\tПолучили new_hidden_delta [-0.03254191  0.03514132  0.02505568  0.02602693  0.02126818  0.00489023\n",
      "  0.01235094 -0.00225488 -0.00374738 -0.04933733] \n",
      "\n",
      "\tТеперь проверим, с последним слоем мы вообще работаем или нет.\n",
      "\tЭто последний слой! Значит просто запоминаем для него полученный только что градиент.\n",
      "\n",
      "\tПолучили: [-0.03254191  0.03514132  0.02505568  0.02602693  0.02126818  0.00489023\n",
      "  0.01235094 -0.00225488 -0.00374738 -0.04933733]\n",
      "Работаем со слоем 3\n",
      "Извлекаем из списка слоев слой №3\n",
      "Целевое слово (\"the\") под индексом: 15\n",
      "\tЭто не самый первый слой!\n",
      "\tСравнили предсказание (layer['pred']) с правдой (one_hot[target]), получили дельту (layer['output_delta']): \n",
      "[ 0.05257341  0.0524182   0.05294842  0.05251555  0.05239898  0.0526549\n",
      "  0.05250486  0.05262644  0.05281424  0.0526735   0.05294401  0.05258932\n",
      "  0.05271803  0.05242483  0.0526049  -0.94754406  0.05245935  0.05256194\n",
      "  0.05311319]\n",
      "\tТеперь рапространим градиент (полученную дельту) на скрытый слой.\n",
      "\tПолучили new_hidden_delta [ 0.01203236  0.01411375  0.03524078 -0.0118242  -0.03740125 -0.02442307\n",
      " -0.04155739 -0.03230097  0.01266078 -0.0361847 ] \n",
      "\n",
      "\tТеперь проверим, с последним слоем мы вообще работаем или нет.\n",
      "\tЭто не последний слой! Значит запоминаем для него полученный только что градиент с добавлением градиента вышестоящего слоя.\n",
      "\n",
      "\tПолучили: [-0.02050955  0.04925506  0.06029646  0.01420273 -0.01613307 -0.01953284\n",
      " -0.02920645 -0.03455585  0.0089134  -0.08552204]\n",
      "Работаем со слоем 2\n",
      "Извлекаем из списка слоев слой №2\n",
      "Целевое слово (\"to\") под индексом: 4\n",
      "\tЭто не самый первый слой!\n",
      "\tСравнили предсказание (layer['pred']) с правдой (one_hot[target]), получили дельту (layer['output_delta']): \n",
      "[ 0.05260904  0.05255498  0.05269478  0.05274347 -0.94757692  0.05274668\n",
      "  0.05247512  0.05258157  0.05273412  0.05273518  0.05280683  0.05260151\n",
      "  0.05279757  0.05240162  0.05265017  0.05249874  0.05251601  0.05254763\n",
      "  0.05288188]\n",
      "\tТеперь рапространим градиент (полученную дельту) на скрытый слой.\n",
      "\tПолучили new_hidden_delta [-0.0301603   0.03593177 -0.02619723 -0.03401027  0.01284376 -0.01655544\n",
      " -0.01563783 -0.01003193 -0.01304366 -0.0135718 ] \n",
      "\n",
      "\tТеперь проверим, с последним слоем мы вообще работаем или нет.\n",
      "\tЭто не последний слой! Значит запоминаем для него полученный только что градиент с добавлением градиента вышестоящего слоя.\n",
      "\n",
      "\tПолучили: [-0.05066985  0.08518683  0.03409923 -0.01980754 -0.00328931 -0.03608828\n",
      " -0.04484428 -0.04458777 -0.00413026 -0.09909384]\n",
      "Работаем со слоем 1\n",
      "Извлекаем из списка слоев слой №1\n",
      "Целевое слово (\"moved\") под индексом: 9\n",
      "\tЭто не самый первый слой!\n",
      "\tСравнили предсказание (layer['pred']) с правдой (one_hot[target]), получили дельту (layer['output_delta']): \n",
      "[ 0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158\n",
      "  0.05263158  0.05263158  0.05263158 -0.94736842  0.05263158  0.05263158\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158\n",
      "  0.05263158]\n",
      "\tТеперь рапространим градиент (полученную дельту) на скрытый слой.\n",
      "\tПолучили new_hidden_delta [-0.03432038 -0.03416436 -0.00564308  0.00622771 -0.0147664   0.02253361\n",
      "  0.03255312  0.02995056 -0.01585152  0.01452716] \n",
      "\n",
      "\tТеперь проверим, с последним слоем мы вообще работаем или нет.\n",
      "\tЭто не последний слой! Значит запоминаем для него полученный только что градиент с добавлением градиента вышестоящего слоя.\n",
      "\n",
      "\tПолучили: [-0.08499023  0.05102247  0.02845616 -0.01357982 -0.01805571 -0.01355467\n",
      " -0.01229116 -0.01463721 -0.01998178 -0.08456668]\n",
      "Работаем со слоем 0\n",
      "Извлекаем из списка слоев слой №0\n",
      "Целевое слово (\"bathroom\") под индексом: 5\n",
      "\tЭто самый первый слой! Значит запоминаем для него полученный для слоя выше градиент (но не забываем распределить его умножением на матрицу current).\n",
      "\tВот эта дельта берется (такая же, как слоем выше): [-0.08499023  0.05102247  0.02845616 -0.01357982 -0.01805571 -0.01355467\n",
      " -0.01229116 -0.01463721 -0.01998178 -0.08456668]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens2 = [tokens[0]]\n",
    "\n",
    "for iteration in range(1):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens2[iteration% len(tokens2)][1:]) # берем очередной отзыв\n",
    "    print(f'ИТЕРАЦИЯ №{iteration}, вектор предложения: {sent} (без первого элемента)')\n",
    "    layers, loss = predict(sent) # получаем список значений на скрытом слое и прогноз\n",
    "    print('Количество слоев (len(layers)):', len(layers))\n",
    "    print('Ошибка (loss)', loss, '\\n')\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))): # [4, 3, 2, 1, 0]\n",
    "        print('Работаем со слоем', layer_idx)\n",
    "        layer = layers[layer_idx] # берем слой (с последнего)\n",
    "        print(f'Извлекаем из списка слоев слой №{layer_idx}')\n",
    "        target = sent[layer_idx - 1]\n",
    "        print(f'Целевое слово (\"{vocab[target]}\") под индексом: {target}')\n",
    "        \n",
    "        if layer_idx > 0: # если не первый слой\n",
    "            print('\\tЭто не самый первый слой!')\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target] # сравниваем предсказание с правдой\n",
    "            print(f\"\\tСравнили предсказание (layer['pred']) с правдой (one_hot[target]), \\\n",
    "получили дельту (layer['output_delta']): \\n{layer['output_delta']}\")\n",
    "            print('\\tТеперь рапространим градиент (полученную дельту) на скрытый слой.')\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.T)\n",
    "            print(f\"\\tПолучили new_hidden_delta\", new_hidden_delta, '\\n')\n",
    "            print('\\tТеперь проверим, с последним слоем мы вообще работаем или нет.')\n",
    "            \n",
    "            if layer_idx == len(layers) - 1 :\n",
    "                print(\"\\tЭто последний слой! Значит просто запоминаем для него полученный только что градиент.\\n\")\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "                print(f\"\\tПолучили: {layer['hidden_delta']}\")\n",
    "            else:\n",
    "                print(\"\\tЭто не последний слой! Значит запоминаем для него \\\n",
    "полученный только что градиент с добавлением градиента вышестоящего слоя.\\n\")\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                layers[layer_idx+1]['hidden_delta'].dot(recurrent.T)\n",
    "                print(f\"\\tПолучили: {layer['hidden_delta']}\")\n",
    "        else: # если первый слой\n",
    "            print('\\tЭто самый первый слой! Значит запоминаем для него полученный для слоя выше градиент \\\n",
    "(но не забываем распределить его умножением на матрицу current).')\n",
    "            print(f\"\\tВот эта дельта берется (такая же, как слоем выше): {layers[layer_idx + 1]['hidden_delta']}\\n\")\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta']\\\n",
    "            .dot(recurrent.T)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корректировка весов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 19.030460861426217\n",
      "Perplexity: 19.005338406741927\n",
      "Perplexity: 18.983630600166556\n",
      "Perplexity: 18.96162976790398\n",
      "Perplexity: 18.93550027081478\n",
      "Perplexity: 18.900274769526536\n",
      "Perplexity: 18.848315746843465\n",
      "Perplexity: 18.76630133653987\n",
      "Perplexity: 18.628225780001614\n",
      "Perplexity: 18.376310435967987\n",
      "Perplexity: 17.85820485139596\n",
      "Perplexity: 16.582238702769008\n",
      "Perplexity: 13.796186842143726\n",
      "Perplexity: 12.413079451432717\n",
      "Perplexity: 11.652984093299983\n",
      "Perplexity: 11.230679026774595\n",
      "Perplexity: 10.993438271591858\n",
      "Perplexity: 10.615148195524222\n",
      "Perplexity: 9.94973814736961\n",
      "Perplexity: 8.927886846032001\n",
      "Perplexity: 7.786784807732955\n",
      "Perplexity: 6.878345922615974\n",
      "Perplexity: 6.323292210757849\n",
      "Perplexity: 6.063338271167899\n",
      "Perplexity: 6.015027161466452\n",
      "Perplexity: 6.084479950256829\n",
      "Perplexity: 6.192234499608798\n",
      "Perplexity: 6.291687898302753\n",
      "Perplexity: 6.339789845237176\n",
      "Perplexity: 6.280486863188851\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iteration% len(tokens)][1:]) # берем очередной отзыв\n",
    "    layers, loss = predict(sent) # получаем список значений на скрытом слое и прогноз\n",
    "#     print(loss)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))): # [4, 3, 2, 1, 0]\n",
    "        layer = layers[layer_idx] # берем слой (с последнего)\n",
    "        target = sent[layer_idx - 1]\n",
    "        \n",
    "        if layer_idx > 0: # если не первый слой\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target] # сравниваем предсказание с правдой\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.T)\n",
    "            \n",
    "            if layer_idx == len(layers) - 1 :\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                layers[layer_idx+1]['hidden_delta'].dot(recurrent.T)\n",
    "        else: # если первый слой\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta']\\\n",
    "            .dot(recurrent.T)\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent)) # обновляем векторное представление предложения\n",
    "    \n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta'] * alpha / float(len(sent)))\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha/float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
    "    if iteration % 1000 == 0:\n",
    "#         print(loss, float(len(sent)))\n",
    "        print(f'Perplexity: {np.exp(loss/len(sent))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sandra', 'moved', 'to', 'the', 'garden']\n",
      "Prev Input: Sandra      True: moved          Pred: is\n",
      "Prev Input: moved       True: to             Pred: to\n",
      "Prev Input: to          True: the            Pred: the\n",
      "Prev Input: the         True: garden         Pred: bedroom\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(50000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iteration% len(tokens)][1:]) # берем очередной отзыв\n",
    "    layers, loss = predict(sent) # получаем список значений на скрытом слое и прогноз\n",
    "#     print(loss)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))): # [4, 3, 2, 1, 0]\n",
    "        layer = layers[layer_idx] # берем слой (с последнего)\n",
    "        target = sent[layer_idx - 1]\n",
    "        \n",
    "        if layer_idx > 0: # если не первый слой\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target] # сравниваем предсказание с правдой\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.T)\n",
    "            \n",
    "            if layer_idx == len(layers) - 1 :\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                layers[layer_idx+1]['hidden_delta'].dot(recurrent.T)\n",
    "        else: # если первый слой\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta']\\\n",
    "            .dot(recurrent.T)\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent)) # обновляем векторное представление предложения\n",
    "    \n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta'] * alpha / float(len(sent)))\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha/float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
    "#     if iteration % 1000 == 0:\n",
    "#         print(loss, float(len(sent)))\n",
    "#         print(f'Perplexity: {np.exp(loss/len(sent))}')\n",
    "\n",
    "sent_index = 4\n",
    "l, _ = predict(words2indices(tokens[sent_index]))\n",
    "\n",
    "print(tokens[sent_index])\n",
    "\n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    inp = tokens[sent_index][i]\n",
    "    true = tokens[sent_index][i+1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print(\"Prev Input: \" + inp + (' ' * (12 - len(inp))) +\\\n",
    "         \"True: \" + true + (\" \" * (15 - len(true))) + \"Pred: \"+ pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
