{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Глава 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 59 68 ... 28 39  8]\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "raw = ''\n",
    "with open('moscow_petushki.txt') as f:\n",
    "    for r in f.readlines():\n",
    "        raw += r # получаем одну большую строку\n",
    "\n",
    "vocab = list(set(raw)) # получаем список уникальных символов\n",
    "word2index = {}\n",
    "for i, word in enumerate (vocab):\n",
    "    word2index[word] = i # каждому символу присваиваем индекс\n",
    "indices = np.array(list(map(lambda x: word2index[x], raw) ) ) # зашифровываем весь текст номерами символов\n",
    "print(indices) # получаем одномерный массив из почти 100 тыс номеров символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                    \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"relu\"):\n",
    "                    ones = Tensor((self.grad.data > 0))\n",
    "                    self.creators[0].backward(self.grad * (ones * self ))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(- self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(- self.data)))\n",
    "                      \n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(- self.data))\n",
    "                      \n",
    "    def relu(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(((self.data > 0) * self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"relu\")\n",
    "        return Tensor(((self.data > 0) * self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        return inp.cross_entropy(target)\n",
    "    \n",
    "class Layer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        # np.random.randn возвращает массив нормально распределенных величин около 0\n",
    "        # размера (n_inputs, n_outputs)\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.params.append(self.weight)\n",
    "        self.params.append(self.bias)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.mm(self.weight)+self.bias.expand(0, len(inp.data))\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for l in self.layers:\n",
    "            params += l.get_params()\n",
    "        return params\n",
    "    \n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.params += self.w_ih.get_params()\n",
    "        self.params += self.w_hh.get_params()\n",
    "        self.params += self.w_ho.get_params()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "    \n",
    "class MSELoss(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.tanh()\n",
    "    \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.sigmoid()\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.params.append(self.weight)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.weight.index_select(inp)\n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, params, alpha=0.1):\n",
    "        self.params = params\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if zero:\n",
    "                p.grad.data *= 0     \n",
    "                \n",
    "    def step(self, zero=True):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(params=model.get_params() + embed.get_params(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.T\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int((n_batches-1)/bptt)\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Все \n",
      "[60 59 68 74 51]\n",
      "[[60 51 21 51 29 28 23 51 22 85 51 74 93 92 68  3 29 51 68 29 80 74 51 68\n",
      "  95 93 51 80 51 35 39 28]\n",
      " [59 21 35 94  1 10 74 28 68 51 21 94 51 80 39 68 30  3 68 21 21 21 29 93\n",
      "  61 63 28 94 92 30 29 30]\n",
      " [68 35 10  1  3 74 40 51  3 41 29 80 43 74 79 43 51 35 39 56 10 30 21 51\n",
      "  30 74 51 90 10 87 39 51]\n",
      " [74 41 74 63 51 51 74 80 29 80 92 21 51 20 51 30 43 51 29 60 80 51  3 39\n",
      "  51 68 68 29 74 13 51 42]\n",
      " [51 28 68 28 29 10 68 63 21 51 28 74 21 80 83 51 51 21 62 60 85 41 80 94\n",
      "  59 43 29  3 51 51 80 80]]\n"
     ]
    }
   ],
   "source": [
    "print(raw[:5])\n",
    "print(indices[:5])\n",
    "print(batched_indices[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    inp = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(inp)\n",
    "        output, hidden = model.forward(rnn_input, hidden)\n",
    "\n",
    "        m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        inp = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "def train(iterations=100):\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "        for batch_i in range(batches_to_train):\n",
    "\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = []\n",
    "            for t in range(bptt):\n",
    "                inp = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(inp)\n",
    "                output, hidden = model.forward(rnn_input, hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                \n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "#                     losses.append(batch_loss)\n",
    "                else:\n",
    "                    loss += batch_loss\n",
    "#                     losses.append(batch_loss + losses[-1])\n",
    "\n",
    "            for loss in losses:\n",
    "                ''\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "\n",
    "            log = \"\\r Iter:\" + str(iteration)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(70, '\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 91/100 - Loss:334.74618404861765-                                                                       \n",
      " Iter:1 - Batch 91/100 - Loss:31.010225517168447- о  а                                                                  \n",
      " Iter:2 - Batch 91/100 - Loss:24.110065888373743- о  с и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и\n",
      " Iter:3 - Batch 91/100 - Loss:21.568881866006735- о  о и и с с и и и и и и и и и и и и и и и и и и и и и и и и и и и и и\n",
      " Iter:4 - Batch 91/100 - Loss:19.666693376799497- о  о и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и\n",
      " Iter:5 - Batch 91/100 - Loss:18.325686052741688- рь о и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и\n",
      " Iter:6 - Batch 91/100 - Loss:17.244541899763634- рь о и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и\n",
      " Iter:7 - Batch 91/100 - Loss:16.405635403860753- рь о и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и и\n",
      " Iter:8 - Batch 91/100 - Loss:15.700455971878668- рь о на на на на на на на на на на на на на на на на на на на на на на\n",
      " Iter:9 - Batch 91/100 - Loss:15.002136526225177- рь о на на на на на на на на на на на на на на на на на на на на на на\n",
      " Iter:10 - Batch 91/100 - Loss:14.415797469912563- рь о на на на на на на на на на на на на на на на на на на на на на на\n",
      " Iter:11 - Batch 91/100 - Loss:13.915183277432347-  ь о на на на на на на на на на на на на на на на на на на на на на на\n",
      " Iter:12 - Batch 91/100 - Loss:13.430288222646771  ь о на на на на на на на на на на на на на на на на на на на на на на\n",
      " Iter:13 - Batch 91/100 - Loss:12.957736515655844  ь у на на на на на на на на на на на на на на на на на на на на на на\n",
      " Iter:14 - Batch 91/100 - Loss:12.518219688756563  ь у и на не на не на не на не на не на не на не на не на не на не на \n",
      " Iter:15 - Batch 91/100 - Loss:12.106003499070908  ь у и с у не на не на не на не на не на не на не на не на не на не на\n",
      " Iter:16 - Batch 91/100 - Loss:11.709378150008808  ь у и се ни не не ни не не ни не не ни не не ни не не ни не не ни не \n",
      " Iter:17 - Batch 91/100 - Loss:11.328409522296605  ь у и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска\n",
      " Iter:18 - Batch 91/100 - Loss:10.962518528613595 ь у и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска\n",
      " Iter:19 - Batch 91/100 - Loss:10.607157010176543  ь у и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска\n",
      " Iter:20 - Batch 91/100 - Loss:10.259668583470777 ь у и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска\n",
      " Iter:21 - Batch 91/100 - Loss:9.9203191975044859  ь у и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска\n",
      " Iter:22 - Batch 91/100 - Loss:9.5889600909899925 ь у и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска и ска\n",
      " Iter:23 - Batch 91/100 - Loss:9.2680425887489231 ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:24 - Batch 91/100 - Loss:8.959931381923832-  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:25 - Batch 91/100 - Loss:8.663743467530477-  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:26 - Batch 91/100 - Loss:8.377273755173111-  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:27 - Batch 91/100 - Loss:8.098324101640787-  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:28 - Batch 91/100 - Loss:7.825179364747495-  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:29 - Batch 91/100 - Loss:7.5566217258325545  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:30 - Batch 91/100 - Loss:7.291567593711139-  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:31 - Batch 91/100 - Loss:7.0289037014775175  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:32 - Batch 91/100 - Loss:6.7674278111656445  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:33 - Batch 91/100 - Loss:6.5058947052610375  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:34 - Batch 91/100 - Loss:6.2436304459805155  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:35 - Batch 91/100 - Loss:5.9810414888671715  ь не не не не не не не не не не не не не не не не не не не не не не н\n",
      " Iter:36 - Batch 91/100 - Loss:5.7187743186381735  ь не ни макания душиния душиния душиния душиния душиния душиния душин\n",
      " Iter:37 - Batch 91/100 - Loss:5.4571629998881725 ь не ни макания душиния душиния душиния душиния душиния душиния душин\n",
      " Iter:38 - Batch 91/100 - Loss:5.197526170756715-  ь не ни мину у расскания и сказа макания душинскиния и сказа макания \n",
      " Iter:39 - Batch 91/100 - Loss:4.942496197291952-  ь не ни мину у расскания и сказа макания душинскиния и сказа макания \n",
      " Iter:40 - Batch 91/100 - Loss:4.6943138694494715  а ни макания душинскиния и сказа макания душинскиния и сказа макания \n",
      " Iter:41 - Batch 91/100 - Loss:4.454262076310897  а ни макания душики и сказа макания душики и сказа макания душики и с\n",
      " Iter:42 - Batch 91/100 - Loss:4.2226706824276535  а ни маканить и сем мако ни мину у рассканики ни мину у рассканики ни\n",
      " Iter:43 - Batch 91/100 - Loss:3.9989278171480596-  а ни мако ни минумить и ског. «Ние, дакик мину у рассканики ни минуми\n",
      " Iter:44 - Batch 91/100 - Loss:3.7815999064451267-  а ни мако ни минумить и ског. «Ние, дакик минумить и ског. «Ние, даки\n",
      " Iter:45 - Batch 91/100 - Loss:3.5695220817667583-  а ни мако ни от ум минумить и ског. «Ние, дакик минумить и ског. «Ние\n",
      " Iter:46 - Batch 91/100 - Loss:3.3643812602600365-  а ни мако ни от ум минумить и ског. «Ние» Я отчера могла и ског. «Ние\n",
      " Iter:47 - Batch 91/100 - Loss:3.1696734611759214  а ни мако ни от ум минумить и сем душик минумай, конем и ског. «Ние» \n",
      " Iter:48 - Batch 91/100 - Loss:2.9859812904590686-  а ни мако ни от умность и ског. «Ние» Я отчера могла и скогда минумит\n",
      " Iter:49 - Batch 91/100 - Loss:2.8128075962133614  а ни мако ни от умность и сем душик минумай, конем и сем минумить и с\n",
      " Iter:50 - Batch 91/100 - Loss:2.6500194639348524  а ни мако ни от уна глаза могла и сказаресь и сем минумить и сем мину\n",
      " Iter:51 - Batch 91/100 - Loss:2.4984758568023335-  а ни мако ни от уна глаза могла и скажел! «Ние, до у расскажел! «Ние,\n",
      " Iter:52 - Batch 91/100 - Loss:2.3591455833979493  а не мы не умное гла ил умроника, до у расскажел! «Ние, до у расскаже\n",
      " Iter:53 - Batch 91/100 - Loss:2.2315305455395484-  а не мы не умноет это много не умноет это много не умноет это много н\n",
      " Iter:54 - Batch 91/100 - Loss:2.1146755488674587-  а не мы не умноет это много не умноет это много не умноет это много н\n",
      " Iter:55 - Batch 91/100 - Loss:2.0077672762887016  а не мы не умноет это много не умноет это сто нее в тимное: «Ние» Я н\n",
      " Iter:56 - Batch 91/100 - Loss:1.9102574689863518  а не мы не умноет это сто нее в конец, когда ерим? ить и сем минумай,\n",
      " Iter:57 - Batch 91/100 - Loss:1.8215345130993574-  а не мы не умноет это как минул умность и сем минумай, конем и как ми\n",
      " Iter:58 - Batch 91/100 - Loss:1.7405230724747387  а не мы не умноет это как минул умность и сем сум и помно! Но уналост\n",
      " Iter:59 - Batch 91/100 - Loss:1.6661781607589246  Чик же могла итить и заной ерем, а то нее всем душик же мы не умноет \n",
      " Iter:60 - Batch 91/100 - Loss:1.5987352843555583-  Чик же могла итить и заной ерем, а то нее всем душик же мы глаза моет\n",
      " Iter:61 - Batch 91/100 - Loss:1.5383403042488986-  Чик же моглай расскажелись и семиче: Я, отчера моет и поммово  ЯНе ум\n",
      " Iter:62 - Batch 91/100 - Loss:1.4833162071381947-  Чик же моглай расскажелись и как минул сдел с тельши мак мот душик же\n",
      " Iter:63 - Batch 91/100 - Loss:1.4341015159140706-  Чиль и как минул сдел с тельши мак мот душик же моглай ценя остельска\n",
      " Iter:64 - Batch 91/100 - Loss:1.3905335442040854 о, логда меня он минул с безза денерь мако ним душик же моглай ценя ос\n",
      " Iter:65 - Batch 91/100 - Loss:1.3518915290400542 о, логда меня он минул с безза денерь мако пошел!.  Вонкого скажелись \n",
      " Iter:66 - Batch 91/100 - Loss:1.3177884447247186- о, логда меня он минул с безза денерь меня он минул с безза денерь мен\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:67 - Batch 91/100 - Loss:1.2874395071333748 оло. Бо ум ум меня он минул с безза денерь меня он минул с безза денер\n",
      " Iter:68 - Batch 91/100 - Loss:1.2609297628509024 оло. Бо ум ум меня он мину онитрани ер, не ну тредов. Нух оди разут, к\n",
      " Iter:69 - Batch 91/100 - Loss:1.2371934313979282- оло. Бо ум ум меня он мину онитрани ер, не ну тредные глаза моет и пом\n",
      " Iter:70 - Batch 91/100 - Loss:1.2162139094021083- оло. Бо у скогда ериммоет это концы когда меня он мину онитрани ер, не\n",
      " Iter:71 - Batch 91/100 - Loss:1.1979551443946306- оло. Бо у скогдреет это мно… Нел с безза денецать этоймается, не выдо \n",
      " Iter:72 - Batch 91/100 - Loss:1.1817968563697198- оло. Бо у скогдреет это мно… Нел с безза денецать этоймается, не выдош\n",
      " Iter:73 - Batch 91/100 - Loss:1.1685111415313436- оло. Бо у свою нассудия и помно… Нух оди разут, Ча не ну тредноет это \n",
      " Iter:74 - Batch 91/100 - Loss:1.1553327403266658- оло. Бо у скогдреет это мно… Нел с безза денецать интеллдет ду ер, но \n",
      " Iter:75 - Batch 91/100 - Loss:1.1440420593224399- оло. Бо у свою нассудия и помно… Нух оди разут, Ча не ну тредноет это \n",
      " Iter:76 - Batch 91/100 - Loss:1.1332504965701624- оло. Богдручил, Вет ухроник законько выбо мою ная душик безли он мину \n",
      " Iter:77 - Batch 91/100 - Loss:1.1241123546538012- оло. Богдручил, Вет ухроник заканае было ни расски все чемпом минул с \n",
      " Iter:78 - Batch 91/100 - Loss:1.1156926460713028- оло. Богдручил, а не стебе у развое тредов. Нух оди свои меся даже но \n",
      " Iter:79 - Batch 91/100 - Loss:1.1086161440231888-  ль птой челосто не заней, когда мие, ду, конем су не у на сто нед, я \n",
      " Iter:80 - Batch 91/100 - Loss:1.1016359219049414-  ль…  ЯНо у свою нассудия, не выдоши сказал них, да не ду, когда меня \n",
      " Iter:81 - Batch 91/100 - Loss:1.0957727050367894  ль…  ЯНо уна сто нед, я сам сумноет это как то б бы меня он мину расс\n",
      " Iter:82 - Batch 91/100 - Loss:1.0901641207943798-  ль…  ЯНо уна сто нед, я сам сумноет это как то б бы меня он мину расс\n",
      " Iter:83 - Batch 91/100 - Loss:1.0851631524023835-  ли он минул с только фуходиния дене, Бонть пятновал сдои нее, до вере\n",
      " Iter:84 - Batch 91/100 - Loss:1.0805276817625087  ли он минул с только фуходиния дене, Бонть пятновал сдои нее, до вере\n",
      " Iter:85 - Batch 91/100 - Loss:1.0761102143405314-  ли он минул с беззал, и беззал, и беззал, и беззал, и беззал, и безза\n",
      " Iter:86 - Batch 91/100 - Loss:1.0724673622474137  ли он миокой илудайтелиния и поминся и какам четушик беззал, и беззал\n",
      " Iter:87 - Batch 91/100 - Loss:1.0686753695329285-  ли он минул с беззалдет сим с тебе я отчера моглай расский цет сам су\n",
      " Iter:88 - Batch 91/100 - Loss:1.0695344453930629-  ли он миокой илудайтелиния и поминся и какам четушик беззал, и беззал\n",
      " Iter:89 - Batch 91/100 - Loss:1.0656743791442322-  ли он минул с беззалдет честь с беззалдет честь с беззалдет честь с б\n",
      " Iter:90 - Batch 91/100 - Loss:1.0603922921492053-  ль…  До – отчерь меня он мину расский цет сам сумноет, конемайтаща от\n",
      " Iter:91 - Batch 91/100 - Loss:1.0574071997420806-  ль…  До – отчерь меня он мину расский цет сам сумноет, конемайтаща от\n",
      " Iter:92 - Batch 91/100 - Loss:1.0546787954887387-  ль…  До – отчерь меня он минул с беззалдет сим с беззало выдоши сказа\n",
      " Iter:93 - Batch 91/100 - Loss:1.0522792612424767-  ль…  До – отчерь меня он минул с беззалдет сим с беззало выдоши сказа\n",
      " Iter:94 - Batch 91/100 - Loss:1.0499905564317855-  ль…  До – отчерь меня он меня они ппот душе, две но пы кончерал бы ме\n",
      " Iter:95 - Batch 91/100 - Loss:1.0479234244301945-  ль…  До – отчерь меня он меня они ппот душе, две но пы кончерал бы ме\n",
      " Iter:96 - Batch 91/100 - Loss:1.0460389269271817-  лый же я ость стаки ил размай расскогда же голосто не занем и помпода\n",
      " Iter:97 - Batch 91/100 - Loss:1.0442255461794372-  лый же я ость стаки ил размай расскогда же голосто не занем и помпода\n",
      " Iter:98 - Batch 91/100 - Loss:1.0426715199956788-  ь когда миобог. О тоже не сомо. И отчера моглай расский цет сам сумно\n",
      " Iter:99 - Batch 91/100 - Loss:1.0411118789630568-  ь когда миобог. О тоже не сомо. И отчера моглай расскогда еремайтепур\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ё крочил, а не стебе у развое толосто не занемёил, а не стебе у развое топрать – отчера моглай расский цет сам сумно… Не я – отчерь меня он меня они пповалдницу! ОНе чемное, или в ёпросто, о сто к только фуходиния денецать интеллы. Не не стебе у развое тоёё пет ду расский цет сам сумно… Не я – отчерь меня он меня они пповалдниё пет уж вся черезё всем за все сёет илается, нее, а то бы меня он меня они ппоё все выбожьше: Скажа меня он меня они пповёкая не сого свот минул с беззалдет сим с беззало выдоши сёет иё ператьсиё гразут, Чиремно… Нух оди, выдово\n",
      "– Котом и гла и ёни приза божел! или разжи ёзя ду, а тоже бозя не выдоши сказали и пакак то гово\n",
      "– О в чзогогда есть стаки ил размай расскогда же голосто не занемитривали измотаки ёни разжи ёзя был бы меня он меня они ппоё все выбожьше: Скажа меня он меня они ппово\n",
      "– О в ца меня он меня они пповалдницу! ОНе чемное, или в впом умает, а ёечера моглай расскогда же голосто пы у не ду, а тоже бозли двед, лочемно… Не не сёет это кончёлов. Но нее, до верестолько фуходиния деёерь меня он меня они ппот душе, двеё, когда моглай расскогда ербово\n",
      "– О вечайтельсках палосто не занемитривали измотаки меня он меня они пповалдницу! ОНе чемное, или в впом умает, а Семпом умает, а ёечера моглай расскёи ёнтя? или в конё приза божел! или лыбал сёбазжая душик беззалдет сим с беззало выдоши сёыго. «Нительной не сого свот минул с беззалдет сим с беззало выдоши сказали иё пераммевал субак меня они ппово\n",
      "– О в ца меёять – этот чезя заней, когда мие: ду, а тоже бозли двед, лочемноё когда миобог. О тоже не сёет и помновы скующа, душе, дли винул патьёе, дёя объёу. То, лоёмого фе я – отчера моглай расский цет сам сумно… Не я – отчерь меня он меня они пповалдницу! ОНе чемное, или в впом умает, а Семпоё все выёскончемай рассудны… Я отчера моглай расский цет сам сумно… Не я – отчерь меня он меня они ппот душе, две ного фе я – отчера моёбо нед, я – он было ни разное, а тоже бозли двед, ложелся, Вет ухронё отчера моглай расскогда ербово\n",
      "– О вечайтельск\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    inp = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(inp)\n",
    "        output, hidden = model.forward(rnn_input, hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        c = vocab[m]\n",
    "        inp = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пихаёиё бзяж б было выдошил, а Семпом умает, а Семпоё все выёскончемай рассудёя, не выбодвою нассудия, но выдоши сказали интеллю илед, когда миоб бозмалёчелай взях одивалдет это, до всем за все выбожьше: ё встаю лы глаза не сто нед, я сам сумноет, я и сто кончерал. Но уна сто нед, я сам сумёиё в тоже бозли двед, лочемно… Не не стебе у развое толёчалосто не занемитривалиёский взяхёда меня он меня они ппот душе, две ного фе я – отчера моёбо нед, я – он было ни раё крода бодел!.. Не не ёыроши сто нед, я сам сумноет, я и сто кончёлов. Но нее, до верестолько фуходиния денецать интеллы. Не не стё не заресь отчера моглай расскёи в впом умает, а Семпой черелайтай черемносто нед, я сам сумноетё у это, а довёусто не занем и помподвою назаресная душик за грабо мою настеллёё взяходь когда же я отчал, а не стебе у развое топрать – отчера моглай расскогда ербово\n",
      "– О вечайтельсках палосто не занемитривали измотаки меня он меня они ппово\n",
      "– О в ца меня он меня они ппот душе, две ного фе я – отчера моёбо нед, я – они пассудия, но выдоё нет это у свою мерал б было нима, все выдошоб было нима, в стошика, в сердце номо. Догда мискончемё он минул с беззало выдоши сто нед, я сам сумноё какойчиласках палосто не занемитривали измотаки меня он меня они пповёкёае все-только фуходиния денецать интеллы. Не не стебе у развое топрать – отчера моглай расскогда ербово\n",
      "– О вечайтельсках палосто не занемитривали измотаки меня он меня они пповёкая не сомо. И отчера моёбо нед, я – он было ни разное, а тоже бозли двед, ё чемпой осто не занемитривали измотаки меня он меня они ппотё, конец, а тоже бозя не выдоши сказали и паёмога я выпать интеллы. Не не стебе у развое толёчалосто не занемитривали измотаки меня он меня они ппово\n",
      "– О в ёпёмё пятновалдет это у свою мерал б было ё встёее вся дрины моет это какай ты от беззало выдоши сказали интеллю илед, когда миоё не стебе у развое тоёдивали измою на тридиё. Но умиремай ёзяжелай нее, а то было ни разное, а тоже бозли двед, лоёбозли двед, но вещемёельния не в\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
