{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Главы 6 - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Создание первой глубокой нейронной сети: введение в МОР"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение на одном обучающем примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current weights: [ 0.5   0.48 -0.7 ]\n",
      "iteration: 0\n",
      "prediction: -0.2\n",
      "error: [0.04]\n",
      "updated weights: [ 0.52  0.48 -0.68]\n",
      "----------\n",
      "current weights: [ 0.52  0.48 -0.68]\n",
      "iteration: 1\n",
      "prediction: -0.16\n",
      "error: [0.0256]\n",
      "updated weights: [ 0.536  0.48  -0.664]\n",
      "----------\n",
      "current weights: [ 0.536  0.48  -0.664]\n",
      "iteration: 2\n",
      "prediction: -0.128\n",
      "error: [0.01638]\n",
      "updated weights: [ 0.549  0.48  -0.651]\n",
      "----------\n",
      "current weights: [ 0.549  0.48  -0.651]\n",
      "iteration: 3\n",
      "prediction: -0.102\n",
      "error: [0.01049]\n",
      "updated weights: [ 0.559  0.48  -0.641]\n",
      "----------\n",
      "current weights: [ 0.559  0.48  -0.641]\n",
      "iteration: 4\n",
      "prediction: -0.082\n",
      "error: [0.00671]\n",
      "updated weights: [ 0.567  0.48  -0.633]\n",
      "----------\n",
      "current weights: [ 0.567  0.48  -0.633]\n",
      "iteration: 5\n",
      "prediction: -0.066\n",
      "error: [0.00429]\n",
      "updated weights: [ 0.574  0.48  -0.626]\n",
      "----------\n",
      "current weights: [ 0.574  0.48  -0.626]\n",
      "iteration: 6\n",
      "prediction: -0.052\n",
      "error: [0.00275]\n",
      "updated weights: [ 0.579  0.48  -0.621]\n",
      "----------\n",
      "current weights: [ 0.579  0.48  -0.621]\n",
      "iteration: 7\n",
      "prediction: -0.042\n",
      "error: [0.00176]\n",
      "updated weights: [ 0.583  0.48  -0.617]\n",
      "----------\n",
      "current weights: [ 0.583  0.48  -0.617]\n",
      "iteration: 8\n",
      "prediction: -0.034\n",
      "error: [0.00113]\n",
      "updated weights: [ 0.587  0.48  -0.613]\n",
      "----------\n",
      "current weights: [ 0.587  0.48  -0.613]\n",
      "iteration: 9\n",
      "prediction: -0.027\n",
      "error: [0.00072]\n",
      "updated weights: [ 0.589  0.48  -0.611]\n",
      "----------\n",
      "current weights: [ 0.589  0.48  -0.611]\n",
      "iteration: 10\n",
      "prediction: -0.021\n",
      "error: [0.00046]\n",
      "updated weights: [ 0.591  0.48  -0.609]\n",
      "----------\n",
      "current weights: [ 0.591  0.48  -0.609]\n",
      "iteration: 11\n",
      "prediction: -0.017\n",
      "error: [0.0003]\n",
      "updated weights: [ 0.593  0.48  -0.607]\n",
      "----------\n",
      "current weights: [ 0.593  0.48  -0.607]\n",
      "iteration: 12\n",
      "prediction: -0.014\n",
      "error: [0.00019]\n",
      "updated weights: [ 0.595  0.48  -0.605]\n",
      "----------\n",
      "current weights: [ 0.595  0.48  -0.605]\n",
      "iteration: 13\n",
      "prediction: -0.011\n",
      "error: [0.00012]\n",
      "updated weights: [ 0.596  0.48  -0.604]\n",
      "----------\n",
      "current weights: [ 0.596  0.48  -0.604]\n",
      "iteration: 14\n",
      "prediction: -0.009\n",
      "error: [8.e-05]\n",
      "updated weights: [ 0.596  0.48  -0.604]\n",
      "----------\n",
      "current weights: [ 0.596  0.48  -0.604]\n",
      "iteration: 15\n",
      "prediction: -0.007\n",
      "error: [5.e-05]\n",
      "updated weights: [ 0.597  0.48  -0.603]\n",
      "----------\n",
      "current weights: [ 0.597  0.48  -0.603]\n",
      "iteration: 16\n",
      "prediction: -0.006\n",
      "error: [3.e-05]\n",
      "updated weights: [ 0.598  0.48  -0.602]\n",
      "----------\n",
      "current weights: [ 0.598  0.48  -0.602]\n",
      "iteration: 17\n",
      "prediction: -0.005\n",
      "error: [2.e-05]\n",
      "updated weights: [ 0.598  0.48  -0.602]\n",
      "----------\n",
      "current weights: [ 0.598  0.48  -0.602]\n",
      "iteration: 18\n",
      "prediction: -0.004\n",
      "error: [1.e-05]\n",
      "updated weights: [ 0.599  0.48  -0.601]\n",
      "----------\n",
      "current weights: [ 0.599  0.48  -0.601]\n",
      "iteration: 19\n",
      "prediction: -0.003\n",
      "error: [1.e-05]\n",
      "updated weights: [ 0.599  0.48  -0.601]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array([[1, 0, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [1, 0, 1]]).reshape(6,3)\n",
    "\n",
    "walk_vs_stop = np.array([[0],\n",
    "                         [1],\n",
    "                         [0],\n",
    "                         [1],\n",
    "                         [1],\n",
    "                         [0]]).reshape(6,1)\n",
    "\n",
    "inp = streetlights[0,:]\n",
    "goal_prediction = walk_vs_stop[0,:]\n",
    "\n",
    "for iteration in range(20):\n",
    "    prediction = inp.dot(weights)\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    delta = prediction - goal_prediction\n",
    "    weight_delta = inp*delta*alpha\n",
    "    print(f'current weights: {np.round(weights, 3)}')\n",
    "    weights -= weight_delta\n",
    "    print(f'iteration: {np.round(iteration, 3)}')\n",
    "    print(f'prediction: {np.round(prediction, 3)}')\n",
    "    print(f'error: {np.round(error, 5)}')\n",
    "    print(f'updated weights: {np.round(weights, 3)}')\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение на всех обучающих примерах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "prediction: -0.2 error: [0.04] updated weights: [ 0.52  0.48 -0.68]\n",
      "prediction: -0.16 error: [0.0256] updated weights: [ 0.536  0.48  -0.664]\n",
      "prediction: -0.128 error: [0.01638] updated weights: [ 0.549  0.48  -0.651]\n",
      "prediction: -0.102 error: [0.01049] updated weights: [ 0.559  0.48  -0.641]\n",
      "prediction: -0.082 error: [0.00671] updated weights: [ 0.567  0.48  -0.633]\n",
      "prediction: -0.066 error: [0.00429] updated weights: [ 0.574  0.48  -0.626]\n",
      "TOTAL ERROR iter 0\n",
      "----------\n",
      "iteration: 1\n",
      "prediction: -0.052 error: [0.00275] updated weights: [ 0.579  0.48  -0.621]\n",
      "prediction: -0.042 error: [0.00176] updated weights: [ 0.583  0.48  -0.617]\n",
      "prediction: -0.034 error: [0.00113] updated weights: [ 0.587  0.48  -0.613]\n",
      "prediction: -0.027 error: [0.00072] updated weights: [ 0.589  0.48  -0.611]\n",
      "prediction: -0.021 error: [0.00046] updated weights: [ 0.591  0.48  -0.609]\n",
      "prediction: -0.017 error: [0.0003] updated weights: [ 0.593  0.48  -0.607]\n",
      "TOTAL ERROR iter 1\n",
      "----------\n",
      "iteration: 2\n",
      "prediction: -0.014 error: [0.00019] updated weights: [ 0.595  0.48  -0.605]\n",
      "prediction: -0.011 error: [0.00012] updated weights: [ 0.596  0.48  -0.604]\n",
      "prediction: -0.009 error: [8.e-05] updated weights: [ 0.596  0.48  -0.604]\n",
      "prediction: -0.007 error: [5.e-05] updated weights: [ 0.597  0.48  -0.603]\n",
      "prediction: -0.006 error: [3.e-05] updated weights: [ 0.598  0.48  -0.602]\n",
      "prediction: -0.005 error: [2.e-05] updated weights: [ 0.598  0.48  -0.602]\n",
      "TOTAL ERROR iter 2\n",
      "----------\n",
      "iteration: 3\n",
      "prediction: -0.004 error: [1.e-05] updated weights: [ 0.599  0.48  -0.601]\n",
      "prediction: -0.003 error: [1.e-05] updated weights: [ 0.599  0.48  -0.601]\n",
      "prediction: -0.002 error: [1.e-05] updated weights: [ 0.599  0.48  -0.601]\n",
      "prediction: -0.002 error: [0.] updated weights: [ 0.599  0.48  -0.601]\n",
      "prediction: -0.001 error: [0.] updated weights: [ 0.599  0.48  -0.601]\n",
      "prediction: -0.001 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "TOTAL ERROR iter 3\n",
      "----------\n",
      "iteration: 4\n",
      "prediction: -0.001 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.001 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.001 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "TOTAL ERROR iter 4\n",
      "----------\n",
      "iteration: 5\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "prediction: -0.0 error: [0.] updated weights: [ 0.6   0.48 -0.6 ]\n",
      "TOTAL ERROR iter 5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array([[1, 0, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [1, 0, 1]]).reshape(6,3)\n",
    "\n",
    "walk_vs_stop = np.array([[0],\n",
    "                         [1],\n",
    "                         [0],\n",
    "                         [1],\n",
    "                         [1],\n",
    "                         [0]]).reshape(6,1)\n",
    "\n",
    "inp = streetlights[0,:]\n",
    "goal_prediction = walk_vs_stop[0,:]\n",
    "\n",
    "for iteration in range(6):\n",
    "    print(f'iteration: {iteration}')\n",
    "    total_error = 0\n",
    "    for i in range(streetlights.shape[0]):\n",
    "        prediction = inp.dot(weights)\n",
    "        error = (prediction - goal_prediction) ** 2\n",
    "        total_error += error\n",
    "        delta = prediction - goal_prediction\n",
    "        weight_delta = inp*delta*alpha\n",
    "#         print(f'current weights: {np.round(weights, 3)}')\n",
    "        weights -= weight_delta\n",
    "        print(f'prediction: {np.round(prediction, 3)}', \\\n",
    "              f'error: {np.round(error, 5)}', \\\n",
    "              f'updated weights: {np.round(weights, 3)}')\n",
    "#         print('-'*10)\n",
    "    print(f'TOTAL ERROR iter {iteration}')\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Глубокая сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 | Error:  1.41421\n",
      "----------\n",
      "iter: 9 | Error:  0.63423\n",
      "----------\n",
      "iter: 18 | Error:  0.39058\n",
      "----------\n",
      "iter: 27 | Error:  0.12299\n",
      "----------\n",
      "iter: 36 | Error:  0.01498\n",
      "----------\n",
      "iter: 45 | Error:  0.0011\n",
      "----------\n",
      "iter: 54 | Error:  7e-05\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    '''Зануляет узлы отрицательные слоя'''\n",
    "    return (x>0)*x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output > 0\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4 # в скрытом слое 4 узла\n",
    "\n",
    "# входные данные (векторы 1*3)\n",
    "streetlights = np.array([[1, 0, 1], \n",
    "                         [0, 1, 1],\n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1]]).reshape(4,3)\n",
    "# целевая переменная\n",
    "walk_vs_stop = np.array([[1],\n",
    "                         [1],\n",
    "                         [0],\n",
    "                         [0]]).reshape(4,1)\n",
    "\n",
    "# инициализируем рандомные веса от -1 до 1 для обоих слоев. \n",
    "# Отрицательные значения функция relu() обнулит дальше\n",
    "weights_0_1 = 2*np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(streetlights.shape[0]):\n",
    "        layer_0 = streetlights[i,:].reshape(-1, streetlights.shape[1]) # подаем на входной слой первое наблюдение, форма (1,3)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)) # получаем значения для скрытого слоя (1,3) х (3,4) = (1,4)\n",
    "        layer_2 = layer_1.dot(weights_1_2) # получаем ответ сети (1,4) х (4,1) = (1,1)\n",
    "        \n",
    "        layer_2_error += (layer_2 - walk_vs_stop[i,:])[0][0] ** 2 # 'скаляр' (1,1)\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i,:]) # 'скаляр' (1,1)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1) # 'скаляр' (1,1) * веса в строку (1,4) * поэлементно на (1,4)\n",
    "                                                                            # итого (1,4)\n",
    "        weights_1_2 -= layer_1.T.dot(layer_2_delta) * alpha # (4,1) * (1,1) * скаляр alpha. Итого: (4,1)\n",
    "        weights_0_1 -= layer_0.T.dot(layer_1_delta) * alpha # (3,1) * (1,4) * скаляр alpha. Итого: (3,4)\n",
    "        \n",
    "    if iteration % 9 == 0:\n",
    "        print(f\"iter: {iteration}\", \"| Error: \", np.round(layer_2_error, 5))\n",
    "        print('-'*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "layer_1: [[-0.          0.13177044 -0.        ]] layer_1_delta: [[-0.          0.16505257 -0.        ]] \n",
      "layer_2 (res): [[-0.02129555]]\n",
      "\n",
      "layer_1: [[-0. -0. -0.]] layer_1_delta: [[-0.  0. -0.]] \n",
      "layer_2 (res): [[0.]]\n",
      "\n",
      "layer_1: [[-0. -0. -0.]] layer_1_delta: [[0. 0. 0.]] \n",
      "layer_2 (res): [[0.]]\n",
      "\n",
      "layer_1: [[-0. -0. -0.]] layer_1_delta: [[0. 0. 0.]] \n",
      "layer_2 (res): [[0.]]\n",
      "\n",
      "Error:  2.04304\n",
      "----------\n",
      "iter: 1\n",
      "layer_1: [[-0.          0.06574941 -0.        ]] layer_1_delta: [[-0.          0.13588854 -0.        ]] \n",
      "layer_2 (res): [[-0.00885616]]\n",
      "\n",
      "layer_1: [[-0. -0. -0.]] layer_1_delta: [[-0.  0. -0.]] \n",
      "layer_2 (res): [[0.]]\n",
      "\n",
      "layer_1: [[-0. -0. -0.]] layer_1_delta: [[0. 0. 0.]] \n",
      "layer_2 (res): [[0.]]\n",
      "\n",
      "layer_1: [[-0. -0. -0.]] layer_1_delta: [[0. 0. 0.]] \n",
      "layer_2 (res): [[0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверка примера из книги\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    '''Зануляет узлы отрицательные слоя'''\n",
    "    return (x>0)*x\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 3 # в скрытом слое 3 узла\n",
    "\n",
    "# входные данные (векторы 1*3)\n",
    "streetlights = np.array([[1, 0, 1], \n",
    "                         [0, 1, 1],\n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1]]).reshape(4,3)\n",
    "# целевая переменная\n",
    "walk_vs_stop = np.array([[1],\n",
    "                         [1],\n",
    "                         [0],\n",
    "                         [0]]).reshape(4,1)\n",
    "\n",
    "# инициализируем рандомные веса от -1 до 1 для обоих слоев. \n",
    "# Отрицательные значения функция relu() обнулит дальше\n",
    "weights_0_1 = 2*np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "for iteration in range(2):\n",
    "    layer_2_error = 0\n",
    "    print(f\"iter: {iteration}\")\n",
    "    for i in range(streetlights.shape[0]):\n",
    "        layer_0 = streetlights[i,:].reshape(-1, streetlights.shape[1]) # подаем на входной слой первое наблюдение, форма (1,3)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)) # получаем значения для скрытого слоя (1,3) х (3,3) = (1,3)\n",
    "        \n",
    "        layer_2 = layer_1.dot(weights_1_2) # получаем ответ сети (1,3) х (3,1) = (1,1)\n",
    "        \n",
    "        layer_2_error += (layer_2 - walk_vs_stop[i,:])[0][0] ** 2 # 'скаляр' (1,1)\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i,:]) # 'скаляр' (1,1)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1) # 'скаляр' (1,1) * веса в строку (1,3) * поэлементно на (1,4)\n",
    "                                                                            # итого (1,3)\n",
    "        print(f'layer_1: {layer_1} layer_1_delta: {layer_1_delta} \\nlayer_2 (res): {layer_2}\\n')\n",
    "        weights_1_2 -= layer_1.T.dot(layer_2_delta) * alpha # (4,1) * (1,1) * скаляр alpha. Итого: (3,1)\n",
    "        weights_0_1 -= layer_0.T.dot(layer_1_delta) * alpha # (3,1) * (1,4) * скаляр alpha. Итого: (3,3)\n",
    "        \n",
    "    if iteration % 5 == 0:\n",
    "        print(\"Error: \", np.round(layer_2_error, 5))\n",
    "        print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим еще один слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 | Error:  1.90276\n",
      "----------\n",
      "iter: 9 | Error:  0.846\n",
      "----------\n",
      "iter: 18 | Error:  0.64655\n",
      "----------\n",
      "iter: 27 | Error:  0.11332\n",
      "----------\n",
      "iter: 36 | Error:  0.00054\n",
      "----------\n",
      "iter: 45 | Error:  0.0\n",
      "----------\n",
      "iter: 54 | Error:  0.0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    '''Зануляет узлы отрицательные слоя'''\n",
    "    return (x>0)*x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output > 0\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4 # в скрытом слое 4 узла\n",
    "\n",
    "# входные данные (векторы 1*3)\n",
    "streetlights = np.array([[1, 0, 1], \n",
    "                         [0, 1, 1],\n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1]]).reshape(4,3)\n",
    "# целевая переменная\n",
    "walk_vs_stop = np.array([[1],\n",
    "                         [1],\n",
    "                         [0],\n",
    "                         [0]]).reshape(4,1)\n",
    "\n",
    "# инициализируем рандомные веса от -1 до 1 для обоих слоев. \n",
    "# Отрицательные значения функция relu() обнулит дальше\n",
    "weights_0_1 = 2*np.random.random((3, hidden_size)) - 1\n",
    "weights_1_1 = 2*np.random.random((hidden_size, hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(streetlights.shape[0]):\n",
    "        layer_0 = streetlights[i,:].reshape(-1, streetlights.shape[1]) # подаем на входной слой первое наблюдение, форма (1,3)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)) # получаем значения для скрытого слоя (1,3) х (3,4) = (1,4)\n",
    "        layer_1_5 = relu(layer_1.dot(weights_1_1))\n",
    "        layer_2 = layer_1_5.dot(weights_1_2) # получаем ответ сети (1,4) х (4,1) = (1,1)\n",
    "        \n",
    "        layer_2_error += (layer_2 - walk_vs_stop[i,:])[0][0] ** 2 # 'скаляр' (1,1)\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i,:]) # 'скаляр' (1,1)\n",
    "        \n",
    "        layer_1_5_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1_5) # 'скаляр' (1,1) * веса в строку (1,4) * поэлементно на (1,4)\n",
    "\n",
    "        layer_1_delta = layer_1_5_delta.dot(weights_1_1.T)*relu2deriv(layer_1) # 'скаляр' (1,1) * веса в строку (1,4) * поэлементно на (1,4)\n",
    "                                                                            # итого (1,4)\n",
    "        weights_1_2 -= layer_1_5.T.dot(layer_2_delta) * alpha # (4,1) * (1,1) * скаляр alpha. Итого: (4,1)\n",
    "        weights_1_1 -= layer_1.T.dot(layer_1_5_delta) * alpha # (4,1) * (1,1) * скаляр alpha. Итого: (4,1)\n",
    "        weights_0_1 -= layer_0.T.dot(layer_1_delta) * alpha # (3,1) * (1,4) * скаляр alpha. Итого: (3,4)\n",
    "        \n",
    "    if iteration % 9 == 0:\n",
    "        print(f\"iter: {iteration}\", \"| Error: \", np.round(layer_2_error, 5))\n",
    "        print('-'*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# трехслойная сеть сошлась быстрее!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Глава 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mnist\n",
    "import mnist\n",
    "import sys, numpy as np\n",
    "\n",
    "x_train = mnist.train_images()[:1000,:,:]\n",
    "y_train = mnist.train_labels()[:1000,...]\n",
    "\n",
    "x_test = mnist.test_images()[:2000,:,:]\n",
    "y_test = mnist.test_labels()[:2000,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Error:0.722 Correct: 0.537\n",
      "Iter:10 Error:0.312 Correct: 0.901\n",
      "Iter:20 Error:0.260 Correct: 0.93\n",
      "Iter:30 Error:0.232 Correct: 0.946\n",
      "Iter:40 Error:0.215 Correct: 0.956\n",
      "Iter:50 Error:0.204 Correct: 0.966\n",
      "Iter:60 Error:0.194 Correct: 0.967\n",
      "Iter:70 Error:0.186 Correct: 0.975\n",
      "Iter:80 Error:0.179 Correct: 0.979\n",
      "Iter:90 Error:0.172 Correct: 0.981\n",
      "Iter:100 Error:0.166 Correct: 0.984\n",
      "Iter:110 Error:0.161 Correct: 0.984\n",
      "Iter:120 Error:0.157 Correct: 0.986\n",
      "Iter:130 Error:0.153 Correct: 0.99\n",
      "Iter:140 Error:0.149 Correct: 0.991\n",
      "Iter:150 Error:0.145 Correct: 0.991\n",
      "Iter:160 Error:0.141 Correct: 0.992\n",
      "Iter:170 Error:0.138 Correct: 0.992\n",
      "Iter:180 Error:0.135 Correct: 0.995\n",
      "Iter:190 Error:0.132 Correct: 0.995\n",
      "Iter:200 Error:0.130 Correct: 0.998\n",
      "Iter:210 Error:0.127 Correct: 0.998\n",
      "Iter:220 Error:0.125 Correct: 0.998\n",
      "Iter:230 Error:0.123 Correct: 0.998\n",
      "Iter:240 Error:0.121 Correct: 0.998\n",
      "Iter:250 Error:0.120 Correct: 0.999\n",
      "Iter:260 Error:0.118 Correct: 0.999\n",
      "Iter:270 Error:0.117 Correct: 0.999\n",
      "Iter:280 Error:0.115 Correct: 0.999\n",
      "Iter:290 Error:0.114 Correct: 0.999\n",
      "Iter:300 Error:0.113 Correct: 0.999\n",
      "Iter:310 Error:0.112 Correct: 0.999\n",
      "Iter:320 Error:0.111 Correct: 0.999\n",
      "Iter:330 Error:0.110 Correct: 0.999\n",
      "Iter:340 Error:0.109 Correct: 1.0\n",
      "Iter:350 Error:0.108 Correct: 1.0\n",
      "--------------------\n",
      "Неплохо. Но покажем сети теперь новые изображения...\n",
      "Test: Error:0.742 Correct: 0.643\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "images, labels = x_train[:1000].reshape(1000, 28*28)/255 , y_train[:1000] # загружаем 1000 изображений и меток (train)\n",
    "one_hot_labels = np.zeros((1000, 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n,l in enumerate(labels):\n",
    "    one_hot_labels[n,l] = 1 # для каждой строки в одном из столбцов ставим 1, соответствующую метке (train)\n",
    "labels = one_hot_labels # переопределяем labels\n",
    "\n",
    "test_images, test_labels = x_test[:,:,:].reshape(-1, 28*28)/255, y_test[:] # загружаем 1000 изображений и меток (test)\n",
    "one_hot_labels_test = np.zeros((x_test.shape[0], 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n, l in enumerate(test_labels): # для каждой строки в одном из столбцов ставим 1, соответствующую метке (test)\n",
    "    one_hot_labels_test[n,l] = 1\n",
    "test_labels = one_hot_labels_test\n",
    "\n",
    "    \n",
    "np.random.seed(1) # фиксируем псевдослучайный генератор случайных чисел\n",
    "\n",
    "relu = lambda x: (x>=0)*x # пишем лямбда-функцию relu (обнуляет отрицательные веса)\n",
    "relu2deriv = lambda x: x >= 0 # пишем лямбда-функцию relu2deriv (обнуляет отрицательные нейроны)\n",
    "\n",
    "# устанавливаем параметры\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 351,        40,          784,              10)\n",
    "\n",
    "# инициализируем случайные веса\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1 # для слоя 1, форма (784, 40)\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1 # для слоя 2, форма (40, 10)\n",
    "\n",
    "for j in range(iterations): # итерируемся\n",
    "    error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "    \n",
    "    for i in range(images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "        layer_0 = images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "        layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                            # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "        \n",
    "        error += np.sum((layer_2 - labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                        # в общей ошибке для j-той итерации\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                            # в общем показателей correct_cnt для j-той итерации\n",
    "        layer_2_delta = (layer_2 - labels[i,:]) # считаем отклонение по выходному слою (1,10)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # считаем отклонение по среднему слою:\n",
    "                                                        # скалярно перемножаем отклонение по выходному слою (1,10)\n",
    "                                                        # на транспонированную матрицу весов weights_1_2 (10,40)\n",
    "                                                        # получаем вектор (1,40)\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta) # корректируем веса для выходного слоя: \n",
    "                                                            # из матрицы текущих весов (40,10) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 1 транспонированный (40,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 2 (1,10)\n",
    "                                                            # получаем (40,10) - такой же формы, как и были наши веса weights_1_2\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta) # корректируем веса для скрытого слоя: \n",
    "                                                            # из матрицы текущих весов (784, 40) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 0 транспонированный (784,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 1 (1,40)\n",
    "                                                            # получаем (784,40) - такой же формы, как и были наши веса weights_0_1\n",
    "    if j % 10 == 0:    \n",
    "        print(\"Iter:\" + str(j) + \" Error:\" + str(error/float(images.shape[0]))[:5] + \\\n",
    "              \" Correct: \" + str(correct_cnt/float(images.shape[0])) )\n",
    "        \n",
    "print('-'*20)\n",
    "print(\"Но покажем сети теперь новые изображения...\")\n",
    "\n",
    "# проверяем на тесте\n",
    "error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "\n",
    "for i in range(test_images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "    layer_0 = test_images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "    layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                            # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "    layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                        # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "\n",
    "    error += np.sum((layer_2 - test_labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                    # в общей ошибке для j-той итерации\n",
    "    correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                        # в общем показателей correct_cnt для j-той итерации\n",
    "print(\"Test:\" + \" Error:\" + str(error/float(test_images.shape[0]))[:5] + \\\n",
    "          \" Correct: \" + str(correct_cnt/float(test_images.shape[0])) )\n",
    "print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем в листинг одновременно результаты обучения и результаты проверки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Error:0.722 Correct: 0.537| Test_Error:0.645 Test Correct: 0.5985\n",
      "Iter:10 Error:0.312 Correct: 0.901| Test_Error:0.469 Test Correct: 0.761\n",
      "Iter:20 Error:0.260 Correct: 0.93| Test_Error:0.461 Test Correct: 0.7635\n",
      "Iter:30 Error:0.232 Correct: 0.946| Test_Error:0.464 Test Correct: 0.7595\n",
      "Iter:40 Error:0.215 Correct: 0.956| Test_Error:0.475 Test Correct: 0.754\n",
      "Iter:50 Error:0.204 Correct: 0.966| Test_Error:0.487 Test Correct: 0.7515\n",
      "Iter:60 Error:0.194 Correct: 0.967| Test_Error:0.499 Test Correct: 0.744\n",
      "Iter:70 Error:0.186 Correct: 0.975| Test_Error:0.511 Test Correct: 0.7365\n",
      "Iter:80 Error:0.179 Correct: 0.979| Test_Error:0.521 Test Correct: 0.7305\n",
      "Iter:90 Error:0.172 Correct: 0.981| Test_Error:0.531 Test Correct: 0.7225\n",
      "Iter:100 Error:0.166 Correct: 0.984| Test_Error:0.540 Test Correct: 0.718\n",
      "Iter:110 Error:0.161 Correct: 0.984| Test_Error:0.549 Test Correct: 0.7135\n",
      "Iter:120 Error:0.157 Correct: 0.986| Test_Error:0.557 Test Correct: 0.711\n",
      "Iter:130 Error:0.153 Correct: 0.99| Test_Error:0.565 Test Correct: 0.7075\n",
      "Iter:140 Error:0.149 Correct: 0.991| Test_Error:0.572 Test Correct: 0.7025\n",
      "Iter:150 Error:0.145 Correct: 0.991| Test_Error:0.578 Test Correct: 0.698\n",
      "Iter:160 Error:0.141 Correct: 0.992| Test_Error:0.585 Test Correct: 0.702\n",
      "Iter:170 Error:0.138 Correct: 0.992| Test_Error:0.591 Test Correct: 0.6975\n",
      "Iter:180 Error:0.135 Correct: 0.995| Test_Error:0.597 Test Correct: 0.6995\n",
      "Iter:190 Error:0.132 Correct: 0.995| Test_Error:0.602 Test Correct: 0.695\n",
      "Iter:200 Error:0.130 Correct: 0.998| Test_Error:0.608 Test Correct: 0.6935\n",
      "Iter:210 Error:0.127 Correct: 0.998| Test_Error:0.616 Test Correct: 0.6895\n",
      "Iter:220 Error:0.125 Correct: 0.998| Test_Error:0.627 Test Correct: 0.6855\n",
      "Iter:230 Error:0.123 Correct: 0.998| Test_Error:0.639 Test Correct: 0.6805\n",
      "Iter:240 Error:0.121 Correct: 0.998| Test_Error:0.651 Test Correct: 0.677\n",
      "Iter:250 Error:0.120 Correct: 0.999| Test_Error:0.662 Test Correct: 0.672\n",
      "Iter:260 Error:0.118 Correct: 0.999| Test_Error:0.670 Test Correct: 0.669\n",
      "Iter:270 Error:0.117 Correct: 0.999| Test_Error:0.678 Test Correct: 0.662\n",
      "Iter:280 Error:0.115 Correct: 0.999| Test_Error:0.686 Test Correct: 0.6585\n",
      "Iter:290 Error:0.114 Correct: 0.999| Test_Error:0.693 Test Correct: 0.6555\n",
      "Iter:300 Error:0.113 Correct: 0.999| Test_Error:0.701 Test Correct: 0.6585\n",
      "Iter:310 Error:0.112 Correct: 0.999| Test_Error:0.708 Test Correct: 0.656\n",
      "Iter:320 Error:0.111 Correct: 0.999| Test_Error:0.716 Test Correct: 0.6515\n",
      "Iter:330 Error:0.110 Correct: 0.999| Test_Error:0.724 Test Correct: 0.6495\n",
      "Iter:340 Error:0.109 Correct: 1.0| Test_Error:0.733 Test Correct: 0.648\n",
      "Iter:350 Error:0.108 Correct: 1.0| Test_Error:0.742 Test Correct: 0.643\n"
     ]
    }
   ],
   "source": [
    "images, labels = x_train[:1000].reshape(1000, 28*28)/255 , y_train[:1000] # загружаем 1000 изображений и меток (train)\n",
    "one_hot_labels = np.zeros((1000, 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n,l in enumerate(labels):\n",
    "    one_hot_labels[n,l] = 1 # для каждой строки в одном из столбцов ставим 1, соответствующую метке (train)\n",
    "labels = one_hot_labels # переопределяем labels\n",
    "\n",
    "test_images, test_labels = x_test[:,:,:].reshape(-1, 28*28)/255, y_test[:] # загружаем 1000 изображений и меток (test)\n",
    "one_hot_labels_test = np.zeros((x_test.shape[0], 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n, l in enumerate(test_labels): # для каждой строки в одном из столбцов ставим 1, соответствующую метке (test)\n",
    "    one_hot_labels_test[n,l] = 1\n",
    "test_labels = one_hot_labels_test\n",
    "\n",
    "    \n",
    "np.random.seed(1) # фиксируем псевдослучайный генератор случайных чисел\n",
    "\n",
    "relu = lambda x: (x>=0)*x # пишем лямбда-функцию relu (обнуляет отрицательные веса)\n",
    "relu2deriv = lambda x: x >= 0 # пишем лямбда-функцию relu2deriv (обнуляет отрицательные нейроны)\n",
    "\n",
    "# устанавливаем параметры\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 351,        40,          784,              10)\n",
    "\n",
    "# инициализируем случайные веса\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1 # для слоя 1, форма (784, 40)\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1 # для слоя 2, форма (40, 10)\n",
    "\n",
    "for j in range(iterations): # итерируемся\n",
    "    error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "    \n",
    "    for i in range(images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "        layer_0 = images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "        layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                            # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "        \n",
    "        error += np.sum((layer_2 - labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                        # в общей ошибке для j-той итерации\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                            # в общем показателей correct_cnt для j-той итерации\n",
    "        layer_2_delta = (layer_2 - labels[i,:]) # считаем отклонение по выходному слою (1,10)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # считаем отклонение по среднему слою:\n",
    "                                                        # скалярно перемножаем отклонение по выходному слою (1,10)\n",
    "                                                        # на транспонированную матрицу весов weights_1_2 (10,40)\n",
    "                                                        # получаем вектор (1,40)\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta) # корректируем веса для выходного слоя: \n",
    "                                                            # из матрицы текущих весов (40,10) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 1 транспонированный (40,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 2 (1,10)\n",
    "                                                            # получаем (40,10) - такой же формы, как и были наши веса weights_1_2\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta) # корректируем веса для скрытого слоя: \n",
    "                                                            # из матрицы текущих весов (784, 40) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 0 транспонированный (784,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 1 (1,40)\n",
    "                                                            # получаем (784,40) - такой же формы, как и были наши веса weights_0_1\n",
    "        \n",
    "    if j % 10 == 0:\n",
    "        \n",
    "    # проверяем на тесте\n",
    "        test_error, test_correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "\n",
    "        for i in range(test_images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "            layer_0 = test_images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                    # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "            layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                                # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                            # в общей ошибке для j-той итерации\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                                # в общем показателей correct_cnt для j-той итерации\n",
    "        print(\"Iter:\" + str(j) + \" Error:\" + str(error/float(images.shape[0]))[:5] + \\\n",
    "              \" Correct: \" + str(correct_cnt/float(images.shape[0])) +\\\n",
    "             \"| Test_Error:\" + str(test_error/float(test_images.shape[0]))[:5] +\\\n",
    "             \" Test Correct: \" + str(test_correct_cnt/float(test_images.shape[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранняя остановка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сократим число итераций до 26, чтобы не дать сети возможности заучить шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Error:0.722 Correct: 0.537\n",
      "Iter:5 Error:0.367 Correct: 0.863\n",
      "Iter:10 Error:0.312 Correct: 0.901\n",
      "Iter:15 Error:0.281 Correct: 0.918\n",
      "Iter:20 Error:0.260 Correct: 0.93\n",
      "Iter:25 Error:0.245 Correct: 0.939\n",
      "--------------------\n",
      "Но покажем сети теперь новые изображения...\n",
      "Test: Error:0.461 Correct: 0.762\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "images, labels = x_train[:1000].reshape(1000, 28*28)/255 , y_train[:1000] # загружаем 1000 изображений и меток (train)\n",
    "one_hot_labels = np.zeros((1000, 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n,l in enumerate(labels):\n",
    "    one_hot_labels[n,l] = 1 # для каждой строки в одном из столбцов ставим 1, соответствующую метке (train)\n",
    "labels = one_hot_labels # переопределяем labels\n",
    "\n",
    "test_images, test_labels = x_test[:,:,:].reshape(-1, 28*28)/255, y_test[:] # загружаем 1000 изображений и меток (test)\n",
    "one_hot_labels_test = np.zeros((x_test.shape[0], 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n, l in enumerate(test_labels): # для каждой строки в одном из столбцов ставим 1, соответствующую метке (test)\n",
    "    one_hot_labels_test[n,l] = 1\n",
    "test_labels = one_hot_labels_test\n",
    "\n",
    "    \n",
    "np.random.seed(1) # фиксируем псевдослучайный генератор случайных чисел\n",
    "\n",
    "relu = lambda x: (x>=0)*x # пишем лямбда-функцию relu (обнуляет отрицательные веса)\n",
    "relu2deriv = lambda x: x >= 0 # пишем лямбда-функцию relu2deriv (обнуляет отрицательные нейроны)\n",
    "\n",
    "# устанавливаем параметры\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 26,        40,          784,              10)\n",
    "\n",
    "# инициализируем случайные веса\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1 # для слоя 1, форма (784, 40)\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1 # для слоя 2, форма (40, 10)\n",
    "\n",
    "for j in range(iterations): # итерируемся\n",
    "    error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "    \n",
    "    for i in range(images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "        layer_0 = images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "        layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                            # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "        \n",
    "        error += np.sum((layer_2 - labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                        # в общей ошибке для j-той итерации\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                            # в общем показателей correct_cnt для j-той итерации\n",
    "        layer_2_delta = (layer_2 - labels[i,:]) # считаем отклонение по выходному слою (1,10)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # считаем отклонение по среднему слою:\n",
    "                                                        # скалярно перемножаем отклонение по выходному слою (1,10)\n",
    "                                                        # на транспонированную матрицу весов weights_1_2 (10,40)\n",
    "                                                        # получаем вектор (1,40)\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta) # корректируем веса для выходного слоя: \n",
    "                                                            # из матрицы текущих весов (40,10) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 1 транспонированный (40,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 2 (1,10)\n",
    "                                                            # получаем (40,10) - такой же формы, как и были наши веса weights_1_2\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta) # корректируем веса для скрытого слоя: \n",
    "                                                            # из матрицы текущих весов (784, 40) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 0 транспонированный (784,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 1 (1,40)\n",
    "                                                            # получаем (784,40) - такой же формы, как и были наши веса weights_0_1\n",
    "    if j % 5 == 0:    \n",
    "        print(\"Iter:\" + str(j) + \" Error:\" + str(error/float(images.shape[0]))[:5] + \\\n",
    "              \" Correct: \" + str(correct_cnt/float(images.shape[0])) )\n",
    "        \n",
    "print('-'*20)\n",
    "print(\"Но покажем сети теперь новые изображения...\")\n",
    "\n",
    "# проверяем на тесте\n",
    "error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "\n",
    "for i in range(test_images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "    layer_0 = test_images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "    layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                            # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "    layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                        # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "\n",
    "    error += np.sum((layer_2 - test_labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                    # в общей ошибке для j-той итерации\n",
    "    correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                        # в общем показателей correct_cnt для j-той итерации\n",
    "print(\"Test:\" + \" Error:\" + str(error/float(test_images.shape[0]))[:5] + \\\n",
    "          \" Correct: \" + str(correct_cnt/float(test_images.shape[0])) )\n",
    "print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.random.randint(2, size=(1,40)) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.random.randint(2, size=(1,40)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Test_Error:0.674 Test_Accuracy: 0.584| Train_Error:0.891 Train_Accuracy: 0.413\n",
      "Iter:10 Test_Error:0.496 Test_Accuracy: 0.7285| Train_Error:0.472 Train_Accuracy: 0.764\n",
      "Iter:20 Test_Error:0.453 Test_Accuracy: 0.7685| Train_Error:0.430 Train_Accuracy: 0.809\n",
      "Iter:30 Test_Error:0.454 Test_Accuracy: 0.772| Train_Error:0.415 Train_Accuracy: 0.811\n",
      "Iter:40 Test_Error:0.456 Test_Accuracy: 0.7635| Train_Error:0.413 Train_Accuracy: 0.827\n",
      "Iter:50 Test_Error:0.445 Test_Accuracy: 0.777| Train_Error:0.392 Train_Accuracy: 0.836\n",
      "Iter:60 Test_Error:0.446 Test_Accuracy: 0.7885| Train_Error:0.402 Train_Accuracy: 0.836\n",
      "Iter:70 Test_Error:0.446 Test_Accuracy: 0.7725| Train_Error:0.383 Train_Accuracy: 0.857\n",
      "Iter:80 Test_Error:0.443 Test_Accuracy: 0.775| Train_Error:0.386 Train_Accuracy: 0.854\n",
      "Iter:90 Test_Error:0.446 Test_Accuracy: 0.78| Train_Error:0.376 Train_Accuracy: 0.868\n",
      "Iter:100 Test_Error:0.440 Test_Accuracy: 0.768| Train_Error:0.369 Train_Accuracy: 0.864\n",
      "Iter:110 Test_Error:0.446 Test_Accuracy: 0.7695| Train_Error:0.371 Train_Accuracy: 0.868\n",
      "Iter:120 Test_Error:0.437 Test_Accuracy: 0.767| Train_Error:0.353 Train_Accuracy: 0.857\n",
      "Iter:130 Test_Error:0.444 Test_Accuracy: 0.7715| Train_Error:0.352 Train_Accuracy: 0.867\n",
      "Iter:140 Test_Error:0.440 Test_Accuracy: 0.774| Train_Error:0.355 Train_Accuracy: 0.885\n",
      "Iter:150 Test_Error:0.441 Test_Accuracy: 0.7695| Train_Error:0.342 Train_Accuracy: 0.883\n",
      "Iter:160 Test_Error:0.438 Test_Accuracy: 0.7775| Train_Error:0.361 Train_Accuracy: 0.876\n",
      "Iter:170 Test_Error:0.438 Test_Accuracy: 0.77| Train_Error:0.344 Train_Accuracy: 0.889\n",
      "Iter:180 Test_Error:0.432 Test_Accuracy: 0.7765| Train_Error:0.333 Train_Accuracy: 0.892\n",
      "Iter:190 Test_Error:0.443 Test_Accuracy: 0.7765| Train_Error:0.335 Train_Accuracy: 0.898\n",
      "Iter:200 Test_Error:0.444 Test_Accuracy: 0.7715| Train_Error:0.347 Train_Accuracy: 0.893\n",
      "Iter:210 Test_Error:0.439 Test_Accuracy: 0.7655| Train_Error:0.336 Train_Accuracy: 0.894\n",
      "Iter:220 Test_Error:0.437 Test_Accuracy: 0.771| Train_Error:0.325 Train_Accuracy: 0.896\n",
      "Iter:230 Test_Error:0.438 Test_Accuracy: 0.7765| Train_Error:0.321 Train_Accuracy: 0.894\n",
      "Iter:240 Test_Error:0.451 Test_Accuracy: 0.775| Train_Error:0.332 Train_Accuracy: 0.898\n",
      "Iter:250 Test_Error:0.432 Test_Accuracy: 0.79| Train_Error:0.320 Train_Accuracy: 0.899\n",
      "Iter:260 Test_Error:0.428 Test_Accuracy: 0.7865| Train_Error:0.321 Train_Accuracy: 0.899\n",
      "Iter:270 Test_Error:0.417 Test_Accuracy: 0.789| Train_Error:0.312 Train_Accuracy: 0.906\n",
      "Iter:280 Test_Error:0.433 Test_Accuracy: 0.791| Train_Error:0.317 Train_Accuracy: 0.9\n",
      "Iter:290 Test_Error:0.436 Test_Accuracy: 0.782| Train_Error:0.301 Train_Accuracy: 0.908\n",
      "Iter:300 Test_Error:0.432 Test_Accuracy: 0.7885| Train_Error:0.309 Train_Accuracy: 0.914\n"
     ]
    }
   ],
   "source": [
    "images, labels = x_train[:1000].reshape(1000, 28*28)/255 , y_train[:1000] # загружаем 1000 изображений и меток (train)\n",
    "one_hot_labels = np.zeros((1000, 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n,l in enumerate(labels):\n",
    "    one_hot_labels[n,l] = 1 # для каждой строки в одном из столбцов ставим 1, соответствующую метке (train)\n",
    "labels = one_hot_labels # переопределяем labels\n",
    "\n",
    "test_images, test_labels = x_test[:,:,:].reshape(-1, 28*28)/255, y_test[:] # загружаем 1000 изображений и меток (test)\n",
    "one_hot_labels_test = np.zeros((x_test.shape[0], 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n, l in enumerate(test_labels): # для каждой строки в одном из столбцов ставим 1, соответствующую метке (test)\n",
    "    one_hot_labels_test[n,l] = 1\n",
    "test_labels = one_hot_labels_test\n",
    "\n",
    "    \n",
    "np.random.seed(1) # фиксируем псевдослучайный генератор случайных чисел\n",
    "\n",
    "relu = lambda x: (x>=0)*x # пишем лямбда-функцию relu (обнуляет отрицательные веса)\n",
    "relu2deriv = lambda x: x >= 0 # пишем лямбда-функцию relu2deriv (обнуляет отрицательные нейроны)\n",
    "\n",
    "# устанавливаем параметры\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 301, 100, 784, 10)\n",
    "\n",
    "# инициализируем случайные веса\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1 # для слоя 1, форма (784, 40)\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1 # для слоя 2, форма (40, 10)\n",
    "\n",
    "for j in range(iterations): # итерируемся\n",
    "    error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "    \n",
    "    for i in range(images.shape[0]): # бежим по обучающим примерам\n",
    "        \n",
    "        layer_0 = images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "        \n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) # создаем маску вида array([[0, 1, 0, ... 0, 0, 1]]),\n",
    "                                                                # где вероятность единиц и нулей по 50%\n",
    "        layer_1 *= dropout_mask * 2 # применяем маску, но умноженную на 2 \n",
    "                        # (т.к. сигнал от слоя 1 примерно в 2 раза уменьшается после применения маски, поэтому его нужно усилить)\n",
    "        \n",
    "        layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                            # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "        \n",
    "        error += np.sum((layer_2 - labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                        # в общей ошибке для j-той итерации\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                            # в общем показателей correct_cnt для j-той итерации\n",
    "        layer_2_delta = (layer_2 - labels[i,:]) # считаем отклонение по выходному слою (1,10)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # считаем отклонение по среднему слою:\n",
    "                                                        # скалярно перемножаем отклонение по выходному слою (1,10)\n",
    "                                                        # на транспонированную матрицу весов weights_1_2 (10,40)\n",
    "                                                        # получаем вектор (1,40)\n",
    "        layer_1_delta *= dropout_mask # при обратном распространении корректировать обнуленные маской нейроны не следует\n",
    "                                        # поэтому той же маской обнуляем полученные дельты\n",
    "        \n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta) # корректируем веса для выходного слоя: \n",
    "                                                            # из матрицы текущих весов (40,10) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 1 транспонированный (40,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 2 (1,10)\n",
    "                                                            # получаем (40,10) - такой же формы, как и были наши веса weights_1_2\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta) # корректируем веса для скрытого слоя: \n",
    "                                                            # из матрицы текущих весов (784, 40) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 0 транспонированный (784,1)\n",
    "                                                            # скалярно умножаем на отклонение по слою 1 (1,40)\n",
    "                                                            # получаем (784,40) - такой же формы, как и были наши веса weights_0_1\n",
    "    if j % 10 == 0:\n",
    "        \n",
    "        test_error, test_correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "\n",
    "        for i in range(test_images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "            layer_0 = test_images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                    # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "            layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                                # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                            # в общей ошибке для j-той итерации\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                                # в общем показателей correct_cnt для j-той итерации\n",
    "        print(\"Iter:\" + str(j) + \" Test_Error:\" + str(test_error/float(test_images.shape[0]))[:5] +\\\n",
    "             \" Test_Accuracy: \" + str(test_correct_cnt/float(test_images.shape[0])) +\\\n",
    "              \"| Train_Error:\" + str(error/float(images.shape[0]))[:5] + \\\n",
    "              \" Train_Accuracy: \" + str(correct_cnt/float(images.shape[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С пакетным градиентным спуском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [5]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Test_Error:1.113 Test_Accuracy: 0.12| Train_Error:1.766 Train_Accuracy: 0.085\n",
      "Iter:10 Test_Error:0.825 Test_Accuracy: 0.3825| Train_Error:0.957 Train_Accuracy: 0.286\n",
      "Iter:20 Test_Error:0.750 Test_Accuracy: 0.5065| Train_Error:0.802 Train_Accuracy: 0.448\n",
      "Iter:30 Test_Error:0.712 Test_Accuracy: 0.564| Train_Error:0.741 Train_Accuracy: 0.523\n",
      "Iter:40 Test_Error:0.686 Test_Accuracy: 0.602| Train_Error:0.706 Train_Accuracy: 0.57\n",
      "Iter:50 Test_Error:0.667 Test_Accuracy: 0.62| Train_Error:0.674 Train_Accuracy: 0.605\n",
      "Iter:60 Test_Error:0.652 Test_Accuracy: 0.6315| Train_Error:0.656 Train_Accuracy: 0.627\n",
      "Iter:70 Test_Error:0.638 Test_Accuracy: 0.643| Train_Error:0.628 Train_Accuracy: 0.664\n",
      "Iter:80 Test_Error:0.627 Test_Accuracy: 0.645| Train_Error:0.621 Train_Accuracy: 0.663\n",
      "Iter:90 Test_Error:0.617 Test_Accuracy: 0.6525| Train_Error:0.609 Train_Accuracy: 0.672\n",
      "Iter:100 Test_Error:0.609 Test_Accuracy: 0.66| Train_Error:0.609 Train_Accuracy: 0.671\n",
      "Iter:110 Test_Error:0.600 Test_Accuracy: 0.668| Train_Error:0.587 Train_Accuracy: 0.687\n",
      "Iter:120 Test_Error:0.593 Test_Accuracy: 0.6715| Train_Error:0.584 Train_Accuracy: 0.705\n",
      "Iter:130 Test_Error:0.586 Test_Accuracy: 0.6775| Train_Error:0.578 Train_Accuracy: 0.703\n",
      "Iter:140 Test_Error:0.580 Test_Accuracy: 0.6835| Train_Error:0.568 Train_Accuracy: 0.696\n",
      "Iter:150 Test_Error:0.573 Test_Accuracy: 0.6875| Train_Error:0.557 Train_Accuracy: 0.704\n",
      "Iter:160 Test_Error:0.568 Test_Accuracy: 0.7| Train_Error:0.558 Train_Accuracy: 0.707\n",
      "Iter:170 Test_Error:0.563 Test_Accuracy: 0.7015| Train_Error:0.543 Train_Accuracy: 0.736\n",
      "Iter:180 Test_Error:0.558 Test_Accuracy: 0.704| Train_Error:0.544 Train_Accuracy: 0.716\n",
      "Iter:190 Test_Error:0.554 Test_Accuracy: 0.7095| Train_Error:0.537 Train_Accuracy: 0.718\n",
      "Iter:200 Test_Error:0.549 Test_Accuracy: 0.713| Train_Error:0.535 Train_Accuracy: 0.717\n",
      "Iter:210 Test_Error:0.547 Test_Accuracy: 0.7165| Train_Error:0.530 Train_Accuracy: 0.727\n",
      "Iter:220 Test_Error:0.543 Test_Accuracy: 0.715| Train_Error:0.524 Train_Accuracy: 0.721\n",
      "Iter:230 Test_Error:0.539 Test_Accuracy: 0.718| Train_Error:0.519 Train_Accuracy: 0.747\n",
      "Iter:240 Test_Error:0.536 Test_Accuracy: 0.721| Train_Error:0.521 Train_Accuracy: 0.736\n",
      "Iter:250 Test_Error:0.533 Test_Accuracy: 0.721| Train_Error:0.512 Train_Accuracy: 0.744\n",
      "Iter:260 Test_Error:0.531 Test_Accuracy: 0.7225| Train_Error:0.515 Train_Accuracy: 0.732\n",
      "Iter:270 Test_Error:0.528 Test_Accuracy: 0.727| Train_Error:0.503 Train_Accuracy: 0.76\n",
      "Iter:280 Test_Error:0.526 Test_Accuracy: 0.7265| Train_Error:0.513 Train_Accuracy: 0.758\n",
      "Iter:290 Test_Error:0.524 Test_Accuracy: 0.729| Train_Error:0.502 Train_Accuracy: 0.771\n",
      "Iter:300 Test_Error:0.523 Test_Accuracy: 0.7295| Train_Error:0.502 Train_Accuracy: 0.736\n"
     ]
    }
   ],
   "source": [
    "images, labels = x_train[:1000].reshape(1000, 28*28)/255 , y_train[:1000] # загружаем 1000 изображений и меток (train)\n",
    "one_hot_labels = np.zeros((1000, 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n,l in enumerate(labels):\n",
    "    one_hot_labels[n,l] = 1 # для каждой строки в одном из столбцов ставим 1, соответствующую метке (train)\n",
    "labels = one_hot_labels # переопределяем labels\n",
    "\n",
    "test_images, test_labels = x_test[:,:,:].reshape(-1, 28*28)/255, y_test[:] # загружаем 1000 изображений и меток (test)\n",
    "one_hot_labels_test = np.zeros((x_test.shape[0], 10)) # готовим нулевую матрицу. По строкам обучающие примеры, по столбцам метка\n",
    "for n, l in enumerate(test_labels): # для каждой строки в одном из столбцов ставим 1, соответствующую метке (test)\n",
    "    one_hot_labels_test[n,l] = 1\n",
    "test_labels = one_hot_labels_test\n",
    "\n",
    "    \n",
    "np.random.seed(1) # фиксируем псевдослучайный генератор случайных чисел\n",
    "\n",
    "relu = lambda x: (x>=0)*x # пишем лямбда-функцию relu (обнуляет отрицательные веса)\n",
    "relu2deriv = lambda x: x >= 0 # пишем лямбда-функцию relu2deriv (обнуляет отрицательные нейроны)\n",
    "\n",
    "# устанавливаем параметры\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.01, 301, 100, 784, 10)\n",
    "batch_size = 100\n",
    "\n",
    "# инициализируем случайные веса\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1 # для слоя 1, форма (784, 100)\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1 # для слоя 2, форма (100, 10)\n",
    "\n",
    "for j in range(iterations): # итерируемся\n",
    "    error, correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "    \n",
    "    for i in range(round(images.shape[0]/batch_size)): # бежим по обучающим примерам\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1) * batch_size)) # 0-99, 100-199 ...\n",
    "        \n",
    "        layer_0 = images[batch_start:batch_end,:].reshape(batch_size,pixels_per_image) # подаем на входной слой изображение (100,784)\n",
    "        \n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)).reshape(batch_size,batch_size) # скалярное перемножение входного слоя (100,784) на веса (784,100), \n",
    "                                                # получаем (100,100), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) # создаем маску вида array([[0, 1, 0, ... 0, 0, 1]]),\n",
    "                                                                # где вероятность единиц и нулей по 50% (100,100)\n",
    "        layer_1 *= dropout_mask * 2 # применяем маску, но умноженную на 2 \n",
    "                        # (т.к. сигнал от слоя 1 примерно в 2 раза уменьшается после применения маски, поэтому его нужно усилить)\n",
    "        \n",
    "        layer_2 = layer_1.dot(weights_1_2).reshape(batch_size,num_labels) # скалярное перемножение внутреннего слоя (100,100) на веса (100,10), \n",
    "                                            # получаем выходной слой (100,10) с вероятностями каждого лейбла.\n",
    "        \n",
    "        error += np.sum((layer_2 - labels[batch_start:batch_end,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                        # в общей ошибке для j-той итерации\n",
    "#         print(error)\n",
    "        for k in range(batch_size): # итерируемся по примерам в батче\n",
    "            correct_cnt += int(np.argmax(layer_2[k,:]) == np.argmax(labels[batch_start+k,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                                # в общем показателей correct_cnt для j-той итерации\n",
    "        layer_2_delta = (layer_2 - labels[batch_start:batch_end,:])/batch_size # считаем отклонение по выходному слою (100,10)\n",
    "        #print(layer_2_delta.shape)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # считаем отклонение по среднему слою:\n",
    "                                                        # скалярно перемножаем отклонение по выходному слою (100,10)\n",
    "                                                        # на транспонированную матрицу весов weights_1_2 (10,100)\n",
    "                                                        # получаем вектор (100,100)\n",
    "        layer_1_delta *= dropout_mask # при обратном распространении корректировать обнуленные маской нейроны не следует\n",
    "                                        # поэтому той же маской обнуляем полученные дельты\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta) # корректируем веса для выходного слоя: \n",
    "                                                            # из матрицы текущих весов (100,10) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 1 транспонированный (100,100)\n",
    "                                                            # скалярно умножаем на отклонение по слою 2 (100,10)\n",
    "                                                            # получаем (100,10) - такой же формы, как и были наши веса weights_1_2\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta) # корректируем веса для скрытого слоя: \n",
    "                                                            # из матрицы текущих весов (784, 100) вычитаем\n",
    "                                                            # помноженный на скорость обучения слой 0 транспонированный (784,100)\n",
    "                                                            # скалярно умножаем на отклонение по слою 1 (100,100)\n",
    "                                                            # получаем (784,100) - такой же формы, как и были наши веса weights_0_1\n",
    "    if j % 10 == 0:\n",
    "        \n",
    "        test_error, test_correct_cnt = (0.0, 0) # error - RSE, correct_cnt - количество True-Positive ответов\n",
    "\n",
    "        for i in range(test_images.shape[0]): # бежим по обучающим примерам\n",
    "\n",
    "            layer_0 = test_images[i,:].reshape(1,-1) # подаем на входной слой изображение (1,784)\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1)).reshape(1,-1) # скалярное перемножение входного слоя (1,784) на веса (784,40), \n",
    "                                                    # получаем (1,40), затем применяем relu для обнуления нейронов с отриц. знач.\n",
    "            layer_2 = layer_1.dot(weights_1_2).reshape(1,-1) # скалярное перемножение внутреннего слоя (1,40) на веса (40,10), \n",
    "                                                # получаем выходной слой (1,10) с вероятностями каждого лейбла.\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i,:])**2) # учитываем ошибку по i-тому обучающему примеру \n",
    "                                                            # в общей ошибке для j-той итерации\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i,:])) # учитываем True-Positive ответ (если есть)\n",
    "                                                                                # в общем показателей correct_cnt для j-той итерации\n",
    "        print(\"Iter:\" + str(j) + \" Test_Error:\" + str(test_error/float(test_images.shape[0]))[:5] +\\\n",
    "             \" Test_Accuracy: \" + str(test_correct_cnt/float(test_images.shape[0])) +\\\n",
    "              \"| Train_Error:\" + str(error/float(images.shape[0]))[:5] + \\\n",
    "              \" Train_Accuracy: \" + str(correct_cnt/float(images.shape[0])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
